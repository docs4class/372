[{"path":"index.html","id":"about-this-course","chapter":"1 About this course","heading":"1 About this course","text":"website serves headquarters BADM 372 Applied Analytics.Content updated changes made semester, point told change schedule assignment, can come get updated version.Also, website benefited greatly lots free, readily available resources posted web leverage extensively. encourage review resources analytics journey. specifically use great frequency (say loud THANK authors!):R Data ScienceAn Introduction Statistical Learning Applications RData Science Boxstackoverflow.com, example","code":""},{"path":"syllabus.html","id":"syllabus","chapter":"2 Syllabus","heading":"2 Syllabus","text":"Instructor: Tobin TurnerOffice Hours: mutually convenient time arranged email e-mail: jtturner@presby.edu","code":""},{"path":"syllabus.html","id":"course-objectives-and-learning-outcomes","chapter":"2 Syllabus","heading":"2.1 Course Objectives and Learning Outcomes","text":"course designed introduce data science. Students apply statistical knowledge techniques business non-business contexts.end course students able :end course, able …gain insight datagain insight data, reproduciblygain insight data, reproducibly, using modern programming tools techniquesgain insight data, reproducibly collaboratively, using modern programming tools techniquesgain insight data, reproducibly (literate programming version control) collaboratively, using modern programming tools techniquescommunicate results effectivelyThis course focused understanding applying key business analytical concepts. Although text serves useful foundation concepts covered class, simple memorization material text sufficient. Class participation, discussion, application critical.","code":""},{"path":"syllabus.html","id":"text-and-resources","chapter":"2 Syllabus","heading":"2.2 Text and Resources","text":"course website (primary resource)R Data ScienceAn Introduction Statistical Learning Applications RData Science Boxstackoverflow.com, exampleOther free, publicly available datasets publications.","code":""},{"path":"syllabus.html","id":"performance-evaluation-grading","chapter":"2 Syllabus","heading":"2.3 Performance Evaluation (Grading)","text":"Quizzes Assignments - 40%Exam 1 - 20%Exam 2 - 20%Final Exam - 20%","code":""},{"path":"syllabus.html","id":"exams","chapter":"2 Syllabus","heading":"2.3.1 Exams","text":"Exams cover assigned chapters textbook, assigned readings, lectures, class exercises, class discussions, videos, guest speakers. typically allocate time prior exam clearly identify body knowledge test cover answer questions format objectives exam.","code":""},{"path":"syllabus.html","id":"quizzes-dont-miss-class","chapter":"2 Syllabus","heading":"2.3.2 Quizzes – DON’T MISS CLASS","text":"average quizzes assignments comprise Quizzes Assignments - 40% portion final gradeQuizzes Assignments designed prepare exams ensure stay course materialMissed Quizzes Assignments made later. present.Quizzes rule. LISTEN.\n- Missed Quizzes Assignments made later. present.","code":""},{"path":"syllabus.html","id":"final-average","chapter":"2 Syllabus","heading":"2.3.3 Final Average","text":"Final Average Grade\n90-100 \n88-89 B+\n82-87 B+\n80-81 B-\n78-79 C+\n72-77 C+\n70-71 C-\n60-69 D\n59 F\n90-100 A88-89 B+82-87 B+80-81 B-78-79 C+72-77 C+70-71 C-60-69 D59 F","code":""},{"path":"syllabus.html","id":"class-participation","chapter":"2 Syllabus","heading":"2.4 Class Participation:","text":"frequently give readings assignments complete prior next class meeting. expect fully engage material: answer questions, pose questions, provide insightful observations. Keep mind quality important component “participation.” Periodic cold calls take place. also put students “hot seat” occasion. class sessions, may select random group students lead us discussion debate. selection participants announced class begins, everyone expected prepare discussion. Reading assigned chapters articles best way prepare discussion. concerns called class, please see discuss. purpose “hot seat” stress embarrass students, encourage students actively engage material.","code":""},{"path":"syllabus.html","id":"phones","chapter":"2 Syllabus","heading":"2.5 Phones","text":"Phones allowed used class without instructor’s prior consent. need phone class please let know class. Unauthorized use electronic devices may result lowering grade dismissal class. mean .phone thing? mean .","code":""},{"path":"syllabus.html","id":"attendance","chapter":"2 Syllabus","heading":"2.6 Attendance","text":"expected regular punctual class attendance. Students responsible material missed homework assignments made. class missed, notes/homework obtained another student. 15 minutes late, class considered cancelled. 4 absences allowed semester. Exceeding absence policy may result receiving F course. professors roll official roll students present roll taken counted absent. student must miss exam, must work agreeable time instructor take test prior exam given. student misses test due emergency, student must inform instructor soon possible. special cases, instructor may allow student take make-exam.","code":""},{"path":"syllabus.html","id":"accommodations","chapter":"2 Syllabus","heading":"2.7 Accommodations","text":"Presbyterian College committed providing reasonable accommodations students documented disabilities. seeking academic accommodations Americans Disabilities Act, must register Academic Success Office, located 5th Avenue (beside Campus Police). receive accommodations, please obtain proper Accommodations Approval Form office, meet beginning semester discuss may deliver approved accommodations. especially encourage meet well advance actual accommodations provided, may feasible offer immediate accommodations without sufficient advance notice (case tests). can assure discussions remain confidential. Disability Services information located link http://bit.ly/PCdisabilityservicesAdditionally, student’s responsibility give instructor one week’s notice prior instance accommodation required.","code":""},{"path":"syllabus.html","id":"honor-code-and-plagiarism","chapter":"2 Syllabus","heading":"2.8 Honor Code and Plagiarism:","text":"assignments/exams must work. copying use unauthorized assistance treated violation PC’s Honor Code. unsure resources allowed, please ask. Please note text longer 7 words taken source must placed quotations cited. Also, summarizing source must also cited. Using source showing work violation plagiarism honor code.","code":""},{"path":"syllabus.html","id":"first-generation-version","chapter":"2 Syllabus","heading":"2.9 First-Generation Version:","text":"Presby First+ Advocate. support current first-generation students. Presbyterian College, first-generation students neither parent legal guardian graduated four-year higher education institution bachelor’s degree. first-generation college student, please contact . information support first-generation college students campus visit Presby First+ webpage.","code":""},{"path":"syllabus.html","id":"continuing-advocate-version","chapter":"2 Syllabus","heading":"2.10 Continuing Advocate Version","text":"Presby First+ Advocate. committed supporting first-generation students Presbyterian College. Presbyterian College, first-generation students neither parent legal guardian graduated four-year higher education institution bachelor’s degree. first-generation college student, please contact anytime visit office hours. information support first-generation college students campus visit Presby First+ webpage.","code":""},{"path":"our-class-rhythm.html","id":"our-class-rhythm","chapter":"3 Our Class Rhythm","heading":"3 Our Class Rhythm","text":"Monday: Wrap previous topic introduce ’ve pre-read . Chat. Play. Work examples. Make sure topics applies real-life.Wednesday: Work examples. Chat needed. Live best lives. :).Friday: Apply ’ve learned – demonstrate mastery (typically form quiz, lab, assignment). Rinse. Repeat.","code":""},{"path":"end-in-mind.html","id":"end-in-mind","chapter":"4 End in Mind","heading":"4 End in Mind","text":"Dana Simmons: “Can predict students enroll PC?”Christina Miller: ??? Well, can ? ???","code":""},{"path":"schedule.html","id":"schedule","chapter":"5 Schedule","heading":"5 Schedule","text":"tentative schedule, change. best review often stay page may plan accordingly!","code":""},{"path":"schedule.html","id":"spring-2022","chapter":"5 Schedule","heading":"Spring 2022","text":"","code":""},{"path":"lab-1-excercises.html","id":"lab-1-excercises","chapter":"6 Lab 1 Excercises","heading":"6 Lab 1 Excercises","text":"Let’s make sure feel good BADM 371 material.open notes/internet/R4DS/etc., work must .Use starwars data (dplyr package) answer/:tallest individual? Shortest?many homeworlds ?homeworld individuals? Fewest? Average # idividuals per homeworld?Make plot individuals mass x axis height y axis.Put best fit line plot.biggest outlier dataset?Calculate BMI individuals. average BMI individuals?average BMI homeworld?homeworlds greatest percentage individuals BMI’s greater average found #8 ?many individuals missing data? variables missing data?","code":""},{"path":"lab-1-in-rmarkdown.html","id":"lab-1-in-rmarkdown","chapter":"7 Lab 1 in Rmarkdown","heading":"7 Lab 1 in Rmarkdown","text":"","code":""},{"path":"lab-1-in-rmarkdown.html","id":"r-markdown","chapter":"7 Lab 1 in Rmarkdown","heading":"7.1 R Markdown","text":"tallest individual? Shortest?many homeworlds ?homeworld individuals? Fewest? Average # individuals per homeworld?4-6. Make plot individuals mass x axis height y axis. Put best fit line plot. biggest outlier dataset?Calculate BMI individuals. average BMI individuals?Via google: metric system, formula BMI weight kilograms divided height meters squared. Since height commonly measured centimeters, alternate calculation formula, dividing weight kilograms height centimeters squared, multiplying result 10,000, can usedWhat average BMI homeworld?homeworlds greatest percentage individuals BMI’s greater average found #8 ?\nmany individuals missing data? variables missing data?many individuals missing data? variables missing data?Via google: https://stackoverflow.com/questions/22353633/filter--complete-cases--data-frame-using-dplyr-case-wise-deletion","code":"\nlibrary(dplyr)#>    Min. 1st Qu.  Median    Mean 3rd Qu.    Max.    NA's \n#>    66.0   167.0   180.0   174.4   191.0   264.0       6#> # A tibble: 49 × 1\n#>    homeworld \n#>    <chr>     \n#>  1 Tatooine  \n#>  2 Naboo     \n#>  3 Alderaan  \n#>  4 Stewjon   \n#>  5 Eriadu    \n#>  6 Kashyyyk  \n#>  7 Corellia  \n#>  8 Rodia     \n#>  9 Nal Hutta \n#> 10 Bestine IV\n#> # … with 39 more rows#> # A tibble: 49 × 2\n#>    homeworld     n\n#>    <chr>     <int>\n#>  1 Naboo        11\n#>  2 Tatooine     10\n#>  3 <NA>         10\n#>  4 Alderaan      3\n#>  5 Coruscant     3\n#>  6 Kamino        3\n#>  7 Corellia      2\n#>  8 Kashyyyk      2\n#>  9 Mirial        2\n#> 10 Ryloth        2\n#> # … with 39 more rows\n#> # A tibble: 49 × 2\n#>    homeworld          n\n#>    <chr>          <int>\n#>  1 Aleen Minor        1\n#>  2 Bespin             1\n#>  3 Bestine IV         1\n#>  4 Cato Neimoidia     1\n#>  5 Cerea              1\n#>  6 Champala           1\n#>  7 Chandrila          1\n#>  8 Concord Dawn       1\n#>  9 Dathomir           1\n#> 10 Dorin              1\n#> # … with 39 more rows#> # A tibble: 1 × 3\n#>   name                   mass height\n#>   <chr>                 <dbl>  <int>\n#> 1 Jabba Desilijic Tiure  1358    175#> # A tibble: 59 × 4\n#>    name                 BMI height  mass\n#>    <chr>              <dbl>  <int> <dbl>\n#>  1 Luke Skywalker      26.0    172    77\n#>  2 C-3PO               26.9    167    75\n#>  3 R2-D2               34.7     96    32\n#>  4 Darth Vader         33.3    202   136\n#>  5 Leia Organa         21.8    150    49\n#>  6 Owen Lars           37.9    178   120\n#>  7 Beru Whitesun lars  27.5    165    75\n#>  8 R5-D4               34.0     97    32\n#>  9 Biggs Darklighter   25.1    183    84\n#> 10 Obi-Wan Kenobi      23.2    182    77\n#> # … with 49 more rows\n#> # A tibble: 1 × 1\n#>   `mean(BMI)`\n#>         <dbl>\n#> 1        32.0#> # A tibble: 40 × 2\n#>    homeworld  avg.BMI\n#>    <chr>        <dbl>\n#>  1 Nal Hutta    443. \n#>  2 Vulpter       50.9\n#>  3 Kalee         34.1\n#>  4 Bestine IV    34.0\n#>  5 <NA>          32.6\n#>  6 Malastare     31.9\n#>  7 Trandosha     31.3\n#>  8 Tatooine      29.3\n#>  9 Sullust       26.6\n#> 10 Dathomir      26.1\n#> # … with 30 more rows#> # A tibble: 5 × 2\n#>   homeworld  avg.BMI\n#>   <chr>        <dbl>\n#> 1 Nal Hutta    443. \n#> 2 Vulpter       50.9\n#> 3 Kalee         34.1\n#> 4 Bestine IV    34.0\n#> 5 <NA>          32.6#> # A tibble: 29 × 14\n#>    name         height  mass hair_color skin_color eye_color\n#>    <chr>         <int> <dbl> <chr>      <chr>      <chr>    \n#>  1 Luke Skywal…    172    77 blond      fair       blue     \n#>  2 Darth Vader     202   136 none       white      yellow   \n#>  3 Leia Organa     150    49 brown      light      brown    \n#>  4 Owen Lars       178   120 brown, gr… light      blue     \n#>  5 Beru Whites…    165    75 brown      light      blue     \n#>  6 Biggs Darkl…    183    84 black      light      brown    \n#>  7 Obi-Wan Ken…    182    77 auburn, w… fair       blue-gray\n#>  8 Anakin Skyw…    188    84 blond      fair       blue     \n#>  9 Chewbacca       228   112 brown      unknown    blue     \n#> 10 Han Solo        180    80 brown      fair       brown    \n#> # … with 19 more rows, and 8 more variables:\n#> #   birth_year <dbl>, sex <chr>, gender <chr>,\n#> #   homeworld <chr>, species <chr>, films <list>,\n#> #   vehicles <list>, starships <list>\n#> Warning: `funs()` was deprecated in dplyr 0.8.0.\n#> Please use a list of either functions or lambdas: \n#> \n#>   # Simple named list: \n#>   list(mean = mean, median = median)\n#> \n#>   # Auto named with `tibble::lst()`: \n#>   tibble::lst(mean, median)\n#> \n#>   # Using lambdas\n#>   list(~ mean(., trim = .2), ~ median(., na.rm = TRUE))\n#> This warning is displayed once every 8 hours.\n#> Call `lifecycle::last_lifecycle_warnings()` to see where this warning was generated.\n#> # A tibble: 1 × 14\n#>    name height  mass hair_color skin_color eye_color\n#>   <int>  <int> <int>      <int>      <int>     <int>\n#> 1     0      6    28          5          0         0\n#> # … with 8 more variables: birth_year <int>, sex <int>,\n#> #   gender <int>, homeworld <int>, species <int>,\n#> #   films <int>, vehicles <int>, starships <int>"},{"path":"lab-2-pretty-pictures.html","id":"lab-2-pretty-pictures","chapter":"8 Lab 2: Pretty pictures!","heading":"8 Lab 2: Pretty pictures!","text":"Please make sure read understood R4DS Chapter\ndata visulization. Also check Data\nVisualization ggplot2 Cheat Sheet RStudio.\nandThink DEEPLY: able generate good data vislization R important even awesome tools like PowerBI Tableau around?","code":""},{"path":"lab-2-pretty-pictures.html","id":"ggplot-package-and-code","chapter":"8 Lab 2: Pretty pictures!","heading":"8.1 ggplot package and code","text":"","code":"ggplot(data = ___, mapping = aes(x = ___)) +\n  geom_histogram(binwidth = ___) +\n  facet_wrap(~___)\n\nLet's deconstruct this code: \n- `ggplot()` is the function we are using to build our plot, in layers.\n- In the first layer we always define the data frame as the first argument. Then, we define the mappings between the variables in the dataset and the **aes**thetics of the plot (e.g. x and y coordinates, colors, etc.). \n- In the next layer we represent the data with **geom**etric shapes, in this case with a histogram. You should decide what makes a reasonable bin width for the histogram by trying out a few options.\n- In the final layer we facet the data by neighbourhood."},{"path":"lab-2-pretty-pictures.html","id":"packages","chapter":"8 Lab 2: Pretty pictures!","heading":"8.2 Packages","text":"’ll use tidyverse packages analysis, data dsbox package. Run following code Console load packages.","code":"\nlibrary(tidyverse)\nlibrary(dsbox)"},{"path":"lab-2-pretty-pictures.html","id":"excercises","chapter":"8 Lab 2: Pretty pictures!","heading":"8.3 Excercises","text":"Create figures using data sets mpg diamonds needed:","code":""},{"path":"lab-2-pretty-pictures.html","id":"airbnb-listings-in-edinburgh","chapter":"8 Lab 2: Pretty pictures!","heading":"8.4 Airbnb listings in Edinburgh","text":"data comes dsbox package. Recent development Edinburgh regarding growth Airbnb impact housing market means better understanding Airbnb listings needed. Using data provided Airbnb, can explore Airbnb availability prices vary neighborhood.data come Kaggle database. ’s\nmodified better serve goals exploration.","code":""},{"path":"lab-2-pretty-pictures.html","id":"learning-goals","chapter":"8 Lab 2: Pretty pictures!","heading":"8.4.1 Learning goals","text":"goal assignment conduct thorough analysis Airbnb listings Edinburgh (yet?), instead give chance practice workflow, data visualization, interpretation skills.","code":""},{"path":"lab-2-pretty-pictures.html","id":"data","chapter":"8 Lab 2: Pretty pictures!","heading":"8.4.2 Data","text":"dataset ’ll using called edibnb data dsbox package. Run View(edibnb) Console view data data viewer. row dataset represent?Hint: Markdown, ggplot2, dplyr Quick Reference sheets example inline R code might helpful. can access Help menu RStudio.many observations (rows) dataset ? interesting data present? purpose data collected first place? Visit kaggle site needed.column represents variable. can get list variables data frame using names() function. else can find details variables?can find descriptions variables help file dataset, can access running ?edibnb Console.Create faceted histogram facet represents neighborhood displays distribution Airbnb prices neighborhood. histogram may similar (better! example .)Create faceted histogram facet represents neighborhood displays distribution Airbnb prices neighborhood. histogram may similar (better! example .)Create faceted histogram facet represents neighborhood displays distribution Airbnb prices neighborhood. histogram may similar (better! example .)Create faceted histogram facet represents neighborhood displays distribution Airbnb prices neighborhood. histogram may similar (better! example .)Note: plot give warning observations non-finite values price removed. Don’t worry warning, simply means 199 listings data didn’t prices available, can’t plotted.Create similar visualization, time showing distribution review scores (review_scores_rating) across neighborhoods. answer, include brief interpretation Airbnb guests rate properties general neighborhoods compare terms ratings.Create similar visualization, time showing distribution review scores (review_scores_rating) across neighborhoods. answer, include brief interpretation Airbnb guests rate properties general neighborhoods compare terms ratings.Create another informative visualization choosing. prepared share class – although visualization need explaining!Create another informative visualization choosing. prepared share class – although visualization need explaining!","code":"\nnames(edibnb)\n#>  [1] \"id\"                   \"price\"               \n#>  [3] \"neighbourhood\"        \"accommodates\"        \n#>  [5] \"bathrooms\"            \"bedrooms\"            \n#>  [7] \"beds\"                 \"review_scores_rating\"\n#>  [9] \"number_of_reviews\"    \"listing_url\"#> Warning: Removed 199 rows containing non-finite values\n#> (stat_bin)."},{"path":"lab-2-pretty-pictures.html","id":"instructional-staff-employment-trends","chapter":"8 Lab 2: Pretty pictures!","heading":"8.5 Instructional staff employment trends","text":"next dataset instructional staff employee hiring trends 1975 2011.dataset called instructors found dsbox. can find descriptions variables help file dataset, can access running ?instructors Console.American Association University Professors (AAUP) nonprofit membership association faculty academic professionals. report compiled AAUP shows trends instructional staff employees 1975 2011, contains image similar one given .","code":""},{"path":"lab-2-pretty-pictures.html","id":"more-excercises","chapter":"8 Lab 2: Pretty pictures!","heading":"8.6 More Excercises","text":"Recreate graph similar one .Recreate graph similar one .Discuss improve upon visualization main objective communicate proportion part-time faculty gone time compared instructional staff types.Implement improvements provide improved visualization part answer. Also write sentences chose make improvements address main goal stated .Discuss improve upon visualization main objective communicate proportion part-time faculty gone time compared instructional staff types.Implement improvements provide improved visualization part answer. Also write sentences chose make improvements address main goal stated .","code":""},{"path":"lab-2-ggplot-without-dsbox.html","id":"lab-2-ggplot-without-dsbox","chapter":"9 Lab 2 – ggplot without dsbox","heading":"9 Lab 2 – ggplot without dsbox","text":"","code":""},{"path":"lab-2-ggplot-without-dsbox.html","id":"excercises-using-the-data-sets-mpg-or-diamonds","chapter":"9 Lab 2 – ggplot without dsbox","heading":"9.1 Excercises using the data sets mpg or diamonds","text":"Create figures using data sets mpg diamonds needed:","code":""},{"path":"lab-2-ggplot-without-dsbox.html","id":"palmerpenguins","chapter":"9 Lab 2 – ggplot without dsbox","heading":"9.2 palmerpenguins","text":"palmerpenguins realtively new package CRAN, can install CRAN instead Github.Install like normal package. successful installation, can find two datasets attached package – penguins penguins_raw. can check help page (?penguins_raw ?penguins_raw) understand respective datasets.Please make well-labeled, meangingful plot show many missing variables variable dataset. results shoud look something like :Make plot showing count penguins species.Make plot showing count penguins species.Create plot illustrates relationship flipper_length_mm body_mass_g respect species.Create plot illustrates relationship flipper_length_mm body_mass_g respect species.Create plot illustrates relationship flipper_length_mm body_mass_g respect species island.Create plot illustrates relationship flipper_length_mm body_mass_g respect species island.Create plots using new/interesting geoms make sure plots meangiful, imprmfative labels, . possible examples:Create plots using new/interesting geoms make sure plots meangiful, imprmfative labels, . possible examples:","code":""},{"path":"lab-3-coronavirus-visualization-data-wrangling-and-dates.html","id":"lab-3-coronavirus-visualization-data-wrangling-and-dates","chapter":"10 Lab 3: coronavirus visualization, data wrangling, and dates","heading":"10 Lab 3: coronavirus visualization, data wrangling, and dates","text":"package available GitHub updated daily.use coronavirus package use coronavirus::update_data() function keep data current. also dates preformatted can nice.","code":""},{"path":"lab-3-coronavirus-visualization-data-wrangling-and-dates.html","id":"lets-look-like-applied-analytics-superstars-and-make-some-neat-visuals.","chapter":"10 Lab 3: coronavirus visualization, data wrangling, and dates","heading":"10.1 Let’s look like Applied Analytics Superstars and make some neat visuals.","text":"’d recommend always start trying understand bit data.example, summary let us know?Can create visual showing cases time Russia, Spain, US, Venezuela?\nAlso, might filter(cases >= 0) worth using?Can show deaths time Russia, Spain, US, Venezuela? can play geoms make something neat?Now let’s plot COVID rate (# confirmed cases / population). Something like .useful previous illustration?useful previous illustration?Make chart cumulative cases. Something like :Make chart cumulative cases. Something like :little time extra packages, make graph prettier. Try.Now let’s really fun. Let’s illustrate death rates relative confirmed cases. challenging anything ’ve done far lab? ’re going make data tidy.One way play game.Let’s make little table just date, country, deaths (meaningful variable name), count observations coutry just make sure eveything looks nice.Let’s make little table just confirmed cases.Let’s join together. use left_join.Let’s add cumulative statistics well.Now can plot fun stuff.","code":"\nlibrary(coronavirus)\nlibrary(dplyr)\nlibrary(ggplot2)\nhead(coronavirus)\n#>         date province country     lat      long      type\n#> 1 2020-01-22  Alberta  Canada 53.9333 -116.5765 confirmed\n#> 2 2020-01-23  Alberta  Canada 53.9333 -116.5765 confirmed\n#> 3 2020-01-24  Alberta  Canada 53.9333 -116.5765 confirmed\n#> 4 2020-01-25  Alberta  Canada 53.9333 -116.5765 confirmed\n#> 5 2020-01-26  Alberta  Canada 53.9333 -116.5765 confirmed\n#> 6 2020-01-27  Alberta  Canada 53.9333 -116.5765 confirmed\n#>   cases   uid iso2 iso3 code3    combined_key population\n#> 1     0 12401   CA  CAN   124 Alberta, Canada    4413146\n#> 2     0 12401   CA  CAN   124 Alberta, Canada    4413146\n#> 3     0 12401   CA  CAN   124 Alberta, Canada    4413146\n#> 4     0 12401   CA  CAN   124 Alberta, Canada    4413146\n#> 5     0 12401   CA  CAN   124 Alberta, Canada    4413146\n#> 6     0 12401   CA  CAN   124 Alberta, Canada    4413146\n#>   continent_name continent_code\n#> 1  North America             NA\n#> 2  North America             NA\n#> 3  North America             NA\n#> 4  North America             NA\n#> 5  North America             NA\n#> 6  North America             NA\nsummary(coronavirus$cases)\n#>      Min.   1st Qu.    Median      Mean   3rd Qu.      Max. \n#> -30974748         0         0       471        35   1123456\nlibrary(scales)\nlibrary(ggrepel)\nlibrary(glue)\nlibrary(lubridate)#>         date country deaths\n#> 1 2020-01-22  Russia      0\n#> 2 2020-01-23  Russia      0\n#> 3 2020-01-24  Russia      0\n#> 4 2020-01-25  Russia      0\n#> 5 2020-01-26  Russia      0\n#> 6 2020-01-27  Russia      0\n#>     country   n\n#> 1    Russia 631\n#> 2     Spain 629\n#> 3        US 631\n#> 4 Venezuela 630#>         date country confirmed\n#> 1 2020-01-22  Russia         0\n#> 2 2020-01-23  Russia         0\n#> 3 2020-01-24  Russia         0\n#> 4 2020-01-25  Russia         0\n#> 5 2020-01-26  Russia         0\n#> 6 2020-01-27  Russia         0\n#>     country   n\n#> 1    Russia 631\n#> 2     Spain 631\n#> 3        US 631\n#> 4 Venezuela 631#>         date country deaths confirmed\n#> 1 2020-01-22  Russia      0         0\n#> 2 2020-01-23  Russia      0         0\n#> 3 2020-01-24  Russia      0         0\n#> 4 2020-01-25  Russia      0         0\n#> 5 2020-01-26  Russia      0         0\n#> 6 2020-01-27  Russia      0         0\n#>     country   n\n#> 1    Russia 631\n#> 2     Spain 631\n#> 3        US 631\n#> 4 Venezuela 631#>         date country deaths confirmed cumulative_cases\n#> 1 2020-01-22  Russia      0         0                0\n#> 2 2020-01-23  Russia      0         0                0\n#> 3 2020-01-24  Russia      0         0                0\n#> 4 2020-01-25  Russia      0         0                0\n#> 5 2020-01-26  Russia      0         0                0\n#> 6 2020-01-27  Russia      0         0                0\n#>   cumulative_deaths rate\n#> 1                 0    0\n#> 2                 0    0\n#> 3                 0    0\n#> 4                 0    0\n#> 5                 0    0\n#> 6                 0    0#>       date              country              deaths      \n#>  Min.   :2020-01-22   Length:2524        Min.   :   0.0  \n#>  1st Qu.:2020-06-27   Class :character   1st Qu.:   4.0  \n#>  Median :2020-12-02   Mode  :character   Median : 115.0  \n#>  Mean   :2020-12-02                      Mean   : 407.9  \n#>  3rd Qu.:2021-05-09                      3rd Qu.: 526.0  \n#>  Max.   :2021-10-13                      Max.   :4460.0  \n#>                                          NA's   :3       \n#>    confirmed        cumulative_cases   cumulative_deaths\n#>  Min.   :-74347.0   Min.   :       0   Min.   :     0   \n#>  1st Qu.:   418.8   1st Qu.: 7742899   1st Qu.: 13566   \n#>  Median :  6501.0   Median :12723106   Median : 72982   \n#>  Mean   : 22897.5   Mean   :25127794   Mean   : 90759   \n#>  3rd Qu.: 23351.0   3rd Qu.:57407255   3rd Qu.:163245   \n#>  Max.   :303008.0   Max.   :57793207   Max.   :244190   \n#>                                        NA's   :1769     \n#>       rate          \n#>  Min.   :-0.036576  \n#>  1st Qu.: 0.006006  \n#>  Median : 0.013469  \n#>  Mean   : 0.023459  \n#>  3rd Qu.: 0.024058  \n#>  Max.   : 3.840391  \n#>  NA's   :3"},{"path":"project-e1.html","id":"project-e1","chapter":"11 Project (E1)","heading":"11 Project (E1)","text":"","code":""},{"path":"project-e1.html","id":"a-project-to-call-your-own","chapter":"11 Project (E1)","heading":"11.1 A project to call your own","text":"Pick dataset, dataset……something . first Analytics 2 project. Make us proud, nutshell. details .","code":""},{"path":"project-e1.html","id":"may-be-too-long-but-please-do-read","chapter":"11 Project (E1)","heading":"11.2 May be too long, but please do read","text":"project class consist analysis dataset \nchoosing. Please make sure ok choice. dataset may already exist,\nmay collect data using \nsurvey conducting experiment. can choose data based interests\nbased work courses research projects. goal project \ndemonstrate proficiency techniques covered class (\nbeyond, like) apply novel dataset meaningful way.","code":""},{"path":"project-e1.html","id":"data-1","chapter":"11 Project (E1)","heading":"11.3 Data","text":"order greatest chance success project important \nchoose manageable dataset. means data readily accessible large\nenough multiple relationships can explored. , dataset must least 50\nobservations 10 20 variables (exceptions can made must speak \nfirst). dataset’s variables include categorical variables, discrete numerical\nvariables, continuous numerical variables.analyses must done RStudio. using dataset comes format \nhaven’t encountered class, make sure able load RStudio \ncan tricky depending source. trouble ask help late.Reusing datasets class: reuse datasets used examples / homework \nclass.","code":""},{"path":"project-e1.html","id":"components","chapter":"11 Project (E1)","heading":"11.4 Components","text":"","code":""},{"path":"project-e1.html","id":"project-proposal","chapter":"11 Project (E1)","heading":"11.4.1 Project proposal","text":"draft introduction section project well \ndata analysis plan dataset. section 1\npage (excluding figures). can check print preview confirm length.write analysis including visuals must done using R Markdown.","code":""},{"path":"project-e1.html","id":"section-1---introduction","chapter":"11 Project (E1)","heading":"11.4.1.1 Section 1 - Introduction:","text":"introduction introduce general research\nquestion data (came , collected, \ncases, variables, etc.).","code":""},{"path":"project-e1.html","id":"section-2---data-analysis-plan","chapter":"11 Project (E1)","heading":"11.4.1.2 Section 2 - Data analysis plan:","text":"data analysis plan include:outcome (dependent, response, Y) predictor (independent, explanatory, X)\nvariables use answer question.comparison groups use, applicable.preliminary exploratory data analysis, including summary statistics \nvisualizations, along explanation help learn data.\n(can add later work project..)statistical method(s) believe useful answering question(s).\n(can update later work project.)Ideally use least two options: tree methods, linear regression,\nclassification (like logistic regression).results specific statistical methods needed support \nhypothesized answer?","code":""},{"path":"project-e1.html","id":"section-3---data","chapter":"11 Project (E1)","heading":"11.4.1.3 Section 3 - Data:","text":"yuor write , include enough details understand raw data looked like\nincluded.","code":""},{"path":"project-e1.html","id":"project","chapter":"11 Project (E1)","heading":"11.4.2 Project","text":"","code":""},{"path":"project-e1.html","id":"write-up","chapter":"11 Project (E1)","heading":"11.4.2.1 Write up","text":"providing description dataset research question \nintroduction use remainder write showcase arrived \nanswer / answers question using techniques learned \nclass (beyond, ’re feeling adventurous). goal exhaustive\ndata analysis .e., calculate every statistic procedure \nlearned every variable, rather let know proficient \nasking meaningful questions answering results data analysis, \nproficient using R, proficient interpreting \npresenting results. Focus methods help begin answer research\nquestions. apply every statistical procedure learned.\nAlso pay attention presentation. Neatness, coherency, clarity count.write must also include one two page conclusion discussion.\nrequire summary learned research\nquestion along statistical arguments supporting conclusions. Also\ncritique methods provide suggestions improving analysis.\nIssues pertaining reliability validity data, \nappropriateness statistical analysis discussed . \nparagraph differently able start \nproject next going continue\nwork project also included.project open ended. create kind compelling\nvisualization(s) data R.limit tools packages may use, sticking packages learned class (ISLR R4DS)\nrequired. need visualize data . single high quality\nvisualization receive much higher grade large number poor quality\nvisualizations.finalize write , make sure chunks turned \necho = FALSE. Exception: want \nhighlight something specific piece code, ’re welcomed show\nportion. [See : also want copy raw .Rmd file just html output.]can add sections see fit project make sure\nsection called Introduction beginning section called\nConclusion end. rest !","code":""},{"path":"project-e1.html","id":"presentation","chapter":"11 Project (E1)","heading":"11.4.2.2 Presentation","text":"10 minutes maximum.can use software like final presentation, including R Markdown\ncreate slides. isn’t limit many slides can use, just \ntime limit (10 minutes total). Perhaps try ioslides beamer. presentation\njust account everything \ntried (“, , etc.”), instead convey \nchoices made, , found.","code":""},{"path":"project-e1.html","id":"delivarables","chapter":"11 Project (E1)","heading":"11.4.2.3 Delivarables","text":"submission includeRMarkdown file (formatted clearly present code results)HTML fileDataset(s) (csv RData format, /data folder)Presentation (using Keynote/PowerPoint/Google Slides, export PDF put repo, /presentation folder)Style format count assignment, please take time make\nsure everything looks good data code properly formated.","code":""},{"path":"project-e1.html","id":"grading","chapter":"11 Project (E1)","heading":"11.5 Grading","text":"Grading project take account following:Content - quality research /policy question relevancy\ndata questions?Correctness - statistical procedures carried explained correctly?Writing Presentation - quality statistical presentation,\nwriting, explanations?Creativity Critical Thought - project carefully thought ? \nlimitations carefully considered? appear time effort went \nplanning implementation project?general breakdown scoring follows:90%-100% - Outstanding effort. Student understands apply statistical\nconcepts, can put results cogent argument, can identify weaknesses \nargument, can clearly communicate results others.80%-89% - Good effort. Student understands concepts, puts together\nadequate argument, identifies weaknesses argument, communicates\nresults clearly others.70%-79% - Passing effort. Student misunderstanding concepts several\nareas, trouble putting results together cogent argument, communication\nresults sometimes unclear.60%-69% - Struggling effort. Student making effort, misunderstanding\nmany concepts unable put together cogent argument. Communication\nresults unclear.60% - Student making sufficient effort.Late penalty:Late, within 24 hours due date/time: -20% (applies written portion, option presentation later)later: credit","code":""},{"path":"functions.html","id":"functions","chapter":"12 Functions","heading":"12 Functions","text":"","code":""},{"path":"functions.html","id":"writing-functions","chapter":"12 Functions","heading":"12.1 Writing Functions","text":"","code":""},{"path":"functions.html","id":"fahrenheit-to-kelvin","chapter":"12 Functions","heading":"12.1.1 Fahrenheit to Kelvin","text":"\\(k = ((f - 32) * (5 / 9)) + 273.15\\)","code":"\n((32 - 32) * (5 / 9)) + 273.15\n#> [1] 273.15\n((212 - 32) * (5 / 9)) + 273.15\n#> [1] 373.15\n((-42 - 32) * (5 / 9)) + 273.15\n#> [1] 232.0389\nf_k <- function(f_temp) {\n    ((f_temp - 32) * (5 / 9)) + 273.15\n}\nf_k(32)\n#> [1] 273.15\nf_k(212)\n#> [1] 373.15\nf_k(-42)\n#> [1] 232.0389"},{"path":"functions.html","id":"kelvin-to-celsius","chapter":"12 Functions","heading":"12.1.2 Kelvin to Celsius","text":"","code":"\nk_c <- function(temp_k) {\n    temp_c <- temp_k - 273.15\n    return(temp_c)\n}\nk_c(0)\n#> [1] -273.15"},{"path":"functions.html","id":"fahrenheit-to-celsius","chapter":"12 Functions","heading":"12.1.3 Fahrenheit to Celsius","text":"","code":"\nf_c <- function(temp_f) {\n    temp_k <- f_k(temp_f)\n    temp_c <- k_c(temp_k)\n    return(temp_c)\n}\nf_c(32)\n#> [1] 0\nf_c(212)\n#> [1] 100"},{"path":"functions.html","id":"testing-functions","chapter":"12 Functions","heading":"12.2 Testing Functions","text":"","code":"\nlibrary(testthat)\ntestthat::expect_equal(f_c(32), 0)\ntestthat::expect_equal(f_c(212), 100)"},{"path":"functions.html","id":"exercise","chapter":"12 Functions","heading":"12.3 Exercise","text":"happens use NA, Inf, -Inf function?better names give functions wrote?name functions package?","code":""},{"path":"functions.html","id":"checking-values","chapter":"12 Functions","heading":"12.4 Checking values","text":"Calculating weighted meansIf expect lengths ,\ntest function","code":"\nmean_wt <- function(x, w) {\n  sum(x * w) / sum(w)\n}\nmean_wt(1:6, 1:6)\n#> [1] 4.333333\nmean_wt(1:6, 1:3)\n#> [1] 7.666667\nmean_wt <- function(x, w) {\n  if (length(x) != length(w)) {\n    stop(\"`x` and `w` should be the same length\")\n  }\n  sum(x * w) / sum(w)\n}\nmean_wt(1:6, 1:3)\n#> Error in mean_wt(1:6, 1:3): `x` and `w` should be the same length"},{"path":"functions.html","id":"dot-dot-dot","chapter":"12 Functions","heading":"12.5 dot-dot-dot …","text":"Use pass arguments another function inside.can also use force named arguments function.","code":"\nsum_3 <- function(x, y, z) {\n  return(x + y + z)\n}\nsum_3(1, 2, 3)\n#> [1] 6\nsum_3 <- function(x, y, ..., z) {\n  return(x + y + z)\n}\nsum_3(1, 2, z = 3)\n#> [1] 6\nsum_3(1, 2, z = 3)\n#> [1] 6"},{"path":"conditionals.html","id":"conditionals","chapter":"13 Conditionals","heading":"13 Conditionals","text":"","code":""},{"path":"conditionals.html","id":"if-statements","chapter":"13 Conditionals","heading":"13.1 if statements","text":"current function deal missing numbers","code":"\n# make a modification to this function\nk_c <- function(temp_k) {\n    if (temp_k < 0) {\n        warning('you passed in a negative Kelvin number')\n        # stop()\n        return(NA)\n    }\n    temp_c <- temp_k - 273.15\n    return(temp_c)\n}\nk_c(-9)\n#> Warning in k_c(-9): you passed in a negative Kelvin number\n#> [1] NA\nk_c(NA)Error in if (temp_k < 0) { : missing value where TRUE/FALSE needed\nk_c(0)\n#> [1] -273.15"},{"path":"conditionals.html","id":"if-else-statements","chapter":"13 Conditionals","heading":"13.2 If else statements","text":"current function deal missing numbers","code":"\nk_c <- function(temp_k) {\n    if (temp_k < 0) {\n        warning('you passed in a negative Kelvin number')\n        # stop()\n        return(NA)\n    } else {\n        temp_c <- temp_k - 273.15\n        return(temp_c)\n    }\n}\nk_c(-9)\n#> Warning in k_c(-9): you passed in a negative Kelvin number\n#> [1] NA\nk_c(NA)\nk_c(0)\n#> [1] -273.15"},{"path":"conditionals.html","id":"dealing-with-na","chapter":"13 Conditionals","heading":"13.3 Dealing with NA","text":"Re-write function work missing values.Note need make NA check first.use && || short-circuit boolean comparisons.\nalso guarantee value length 1L.\n== also vectorized, use identical() .equal().identical strict. Doesn’t corece types..equal ability set tolerances..equal: compare R objects x y testing ‘near equality’. different, comparison still made extent, report differences returned. use .equal directly expressions—either use isTRUE(.equal(….)) identical appropriate.","code":"\nk_c <- function(temp_k) {\n    if (is.na(temp_k)) {\n        return(NA)\n    } else if (temp_k < 0) {\n        warning('you passed in a negative Kelvin number')\n        # stop()\n        return(NA)\n    } else {\n        temp_c <- temp_k - 273.15\n        return(temp_c)\n    }\n}\nk_c(-9)\n#> Warning in k_c(-9): you passed in a negative Kelvin number\n#> [1] NA\nk_c(NA)\n#> [1] NA\nk_c(0)\n#> [1] -273.15\nidentical(0L, 0)\n#> [1] FALSE\nall.equal(0L, 0)\n#> [1] TRUE\nif (isTRUE(all.equal(0L, 0))) {print(\"Hello\")}\n#> [1] \"Hello\""},{"path":"conditionals.html","id":"fizzbuzz","chapter":"13 Conditionals","heading":"13.4 Fizzbuzz","text":"Check modulo 3 ","code":"\nfizzbuzz <- function(x) {\n  # these two lines check that x is a valid input\n  stopifnot(length(x) == 1)\n  stopifnot(is.numeric(x))\n  if (!(x %% 3) && !(x %% 5)) {\n    \"fizzbuzz\"\n  } else if (!(x %% 3)) {\n    \"fizz\"\n  } else if (!(x %% 5)) {\n    \"buzz\"\n  } else {\n    # ensure that the function returns a character vector\n    as.character(x)\n  }\n}\nfizzbuzz(6)\n#> [1] \"fizz\"\nfizzbuzz2 <- function(x) {\n  # these two lines check that x is a valid input\n  stopifnot(length(x) == 1)\n  stopifnot(is.numeric(x))\n  if (!(x %% 3)) {\n    if (!(x %% 5)) {\n      \"fizzbuzz\"\n    } else {\n      \"fizz\"\n    }\n  } else if (!(x %% 5)) {\n    \"buzz\"\n  } else {\n    # ensure that the function returns a character vector\n    as.character(x)\n  }\n}\nfizzbuzz(6)\n#> [1] \"fizz\""},{"path":"conditionals.html","id":"vectorized-conditionals","chapter":"13 Conditionals","heading":"13.4.1 Vectorized conditionals","text":"","code":"\nlibrary(dplyr)\n#> \n#> Attaching package: 'dplyr'\n#> The following object is masked from 'package:testthat':\n#> \n#>     matches\n#> The following objects are masked from 'package:stats':\n#> \n#>     filter, lag\n#> The following objects are masked from 'package:base':\n#> \n#>     intersect, setdiff, setequal, union\nfizzbuzz_vec <- function(x) {\n  dplyr::case_when(\n    !(x %% 3) & !(x %% 5) ~ \"fizzbuzz\",\n    !(x %% 3) ~ \"fizz\",\n    !(x %% 5) ~ \"buzz\",\n    TRUE ~ as.character(x)\n  )\n}\nfizzbuzz(1:10)\n#> Error in fizzbuzz(1:10): length(x) == 1 is not TRUE\nfizzbuzz_vec(1:10)\n#>  [1] \"1\"    \"2\"    \"fizz\" \"4\"    \"buzz\" \"fizz\" \"7\"    \"8\"   \n#>  [9] \"fizz\" \"buzz\""},{"path":"conditionals.html","id":"multiple-conditions","chapter":"13 Conditionals","heading":"13.4.2 Multiple conditions","text":"","code":"\nif (this) {\n  # do that\n} else if (that) {\n  # do something else\n} else {\n  # \n}"},{"path":"linear-regression.html","id":"linear-regression","chapter":"14 Linear Regression","heading":"14 Linear Regression","text":"resource ISLR chapter 3: linear regression.","code":""},{"path":"linear-regression.html","id":"exercises","chapter":"14 Linear Regression","heading":"14.1 Exercises","text":"Make sure can define terms outloud, words, make sense someone else (?). Actually practice saying defining terms outloud answers make sense:least squares approachconfidence intervalp-valueR2Adjusted R2qualitative predictorcollinearityKNNResidual standard errorF-statisticExplain point Figure 3.1In m1, , variables significant predictors Balance? know?“good” model created m1? know?Add Credit variables model m1. Can find two variables extremely high collinearity? ? know high collinearity? make sense given variables mean?Based model , predict balance individual 40, income $100,000, 16 years education, Asian student?Interpret model output, especially coefficients Income:Education   0.3149:’s going ?","code":"\nlibrary(\"ISLR\")\ndata(Credit)\nattach(Credit)\nhead(Credit)\n#>   ID  Income Limit Rating Cards Age Education Gender\n#> 1  1  14.891  3606    283     2  34        11   Male\n#> 2  2 106.025  6645    483     3  82        15 Female\n#> 3  3 104.593  7075    514     4  71        11   Male\n#> 4  4 148.924  9504    681     3  36        11 Female\n#> 5  5  55.882  4897    357     2  68        16   Male\n#> 6  6  80.180  8047    569     4  77        10   Male\n#>   Student Married Ethnicity Balance\n#> 1      No     Yes Caucasian     333\n#> 2     Yes     Yes     Asian     903\n#> 3      No      No     Asian     580\n#> 4      No      No     Asian     964\n#> 5      No     Yes Caucasian     331\n#> 6      No      No Caucasian    1151\nm1 <- lm(Balance ~ Age + Income + Education, data = Credit)\nsummary(m1)\n#> \n#> Call:\n#> lm(formula = Balance ~ Age + Income + Education, data = Credit)\n#> \n#> Residuals:\n#>     Min      1Q  Median      3Q     Max \n#> -867.14 -343.14  -49.44  316.55 1080.56 \n#> \n#> Coefficients:\n#>             Estimate Std. Error t value Pr(>|t|)    \n#> (Intercept) 348.8115   112.6895   3.095  0.00211 ** \n#> Age          -2.1863     1.2004  -1.821  0.06930 .  \n#> Income        6.2380     0.5877  10.614  < 2e-16 ***\n#> Education     0.8058     6.5254   0.123  0.90179    \n#> ---\n#> Signif. codes:  \n#> 0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n#> \n#> Residual standard error: 407.2 on 396 degrees of freedom\n#> Multiple R-squared:  0.2215, Adjusted R-squared:  0.2156 \n#> F-statistic: 37.56 on 3 and 396 DF,  p-value: < 2.2e-16\nm2 <- lm(Balance ~ Age + Income + Education + Ethnicity + Student, data = Credit)\nsummary(m2)\n#> \n#> Call:\n#> lm(formula = Balance ~ Age + Income + Education + Ethnicity + \n#>     Student, data = Credit)\n#> \n#> Residuals:\n#>     Min      1Q  Median      3Q     Max \n#> -818.77 -322.14  -54.52  315.67  781.45 \n#> \n#> Coefficients:\n#>                    Estimate Std. Error t value Pr(>|t|)    \n#> (Intercept)        336.6241   115.6311   2.911  0.00381 ** \n#> Age                 -1.9756     1.1595  -1.704  0.08922 .  \n#> Income               6.1491     0.5666  10.853  < 2e-16 ***\n#> Education           -1.7606     6.3060  -0.279  0.78024    \n#> EthnicityAsian     -14.2547    55.5240  -0.257  0.79752    \n#> EthnicityCaucasian   8.8839    48.3276   0.184  0.85424    \n#> StudentYes         382.0498    65.6854   5.816 1.25e-08 ***\n#> ---\n#> Signif. codes:  \n#> 0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n#> \n#> Residual standard error: 392.2 on 393 degrees of freedom\n#> Multiple R-squared:  0.2833, Adjusted R-squared:  0.2723 \n#> F-statistic: 25.89 on 6 and 393 DF,  p-value: < 2.2e-16\nm3 <- lm(Balance ~ Income*Education, data = Credit)\nsummary(m3)\n#> \n#> Call:\n#> lm(formula = Balance ~ Income * Education, data = Credit)\n#> \n#> Residuals:\n#>     Min      1Q  Median      3Q     Max \n#> -858.07 -349.99  -56.12  304.51 1083.93 \n#> \n#> Coefficients:\n#>                  Estimate Std. Error t value Pr(>|t|)   \n#> (Intercept)      435.4599   147.1000   2.960  0.00326 **\n#> Income             1.8168     2.4727   0.735  0.46294   \n#> Education        -13.9887    10.5931  -1.321  0.18741   \n#> Income:Education   0.3149     0.1788   1.761  0.07902 . \n#> ---\n#> Signif. codes:  \n#> 0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n#> \n#> Residual standard error: 407.3 on 396 degrees of freedom\n#> Multiple R-squared:  0.2211, Adjusted R-squared:  0.2152 \n#> F-statistic: 37.47 on 3 and 396 DF,  p-value: < 2.2e-16\noptions(scipen=999)\noptions(digits=3)\nlibrary(ISLR)\ndata(\"Credit\")\nattach(Credit)\n#> The following objects are masked from Credit (pos = 3):\n#> \n#>     Age, Balance, Cards, Education, Ethnicity,\n#>     Gender, ID, Income, Limit, Married, Rating,\n#>     Student\n?Credit\n#> starting httpd help server ...\n#>  done\nm1 <- lm(Balance~Income+Education)\nsummary(m1)\n#> \n#> Call:\n#> lm(formula = Balance ~ Income + Education)\n#> \n#> Residuals:\n#>    Min     1Q Median     3Q    Max \n#> -806.2 -349.7  -53.4  330.4 1103.4 \n#> \n#> Coefficients:\n#>             Estimate Std. Error t value            Pr(>|t|)\n#> (Intercept)  236.975     94.767    2.50               0.013\n#> Income         6.050      0.580   10.43 <0.0000000000000002\n#> Education      0.703      6.544    0.11               0.914\n#>                \n#> (Intercept) *  \n#> Income      ***\n#> Education      \n#> ---\n#> Signif. codes:  \n#> 0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n#> \n#> Residual standard error: 408 on 397 degrees of freedom\n#> Multiple R-squared:  0.215,  Adjusted R-squared:  0.211 \n#> F-statistic: 54.4 on 2 and 397 DF,  p-value: <0.0000000000000002\n236.975+(6.050 *100)+(0.703*12)\n#> [1] 850\npredict(m1, newdata = list(Income = 100, Education = 12))\n#>   1 \n#> 850\n\nm2 <- lm(Balance~Income*Education)\nsummary(m2)\n#> \n#> Call:\n#> lm(formula = Balance ~ Income * Education)\n#> \n#> Residuals:\n#>    Min     1Q Median     3Q    Max \n#> -858.1 -350.0  -56.1  304.5 1083.9 \n#> \n#> Coefficients:\n#>                  Estimate Std. Error t value Pr(>|t|)   \n#> (Intercept)       435.460    147.100    2.96   0.0033 **\n#> Income              1.817      2.473    0.73   0.4629   \n#> Education         -13.989     10.593   -1.32   0.1874   \n#> Income:Education    0.315      0.179    1.76   0.0790 . \n#> ---\n#> Signif. codes:  \n#> 0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n#> \n#> Residual standard error: 407 on 396 degrees of freedom\n#> Multiple R-squared:  0.221,  Adjusted R-squared:  0.215 \n#> F-statistic: 37.5 on 3 and 396 DF,  p-value: <0.0000000000000002\n\n435.460+(1.817*100)+(-13.989*12)+(0.315*100*12)\n#> [1] 827\npredict(m2, newdata = list(Income = 100, Education = 12))\n#>   1 \n#> 827"},{"path":"tree-based-methods.html","id":"tree-based-methods","chapter":"15 Tree-Based Methods","heading":"15 Tree-Based Methods","text":"resource ISLR Chapter 8: Tree-Based Methods.Make sure can exlain terms/ideas/figures outloud, words, make sense someone else (?). Actually practice exlaining terms/ideas/figures outloud answers make sense:Regression vs. classification treesUnderstand Figure 8.1, Figure 8.2, Algorithm 8.1, Figure 8.3, Figure 8.4, Figure 8.5, Figure 8.6, Figure 8.7Be able explain +’s -’s trees (see section 8.1.4)Understand “combining large number trees\ncan often result dramatic improvements prediction accuracy, expense loss interpretation.”top-, greedy approach (aka recursive binary splitting)tree pruning subtreescost complexity pruning (aka weakest link pruning)baggingrandom forestsboostingBayesian additive regression trees","code":""},{"path":"chapter-8-lab-decision-trees.html","id":"chapter-8-lab-decision-trees","chapter":"16 Chapter 8 Lab: Decision Trees","heading":"16 Chapter 8 Lab: Decision Trees","text":"**Check Video: StatsLearning Lect10 R trees 111213Or Click : !!!","code":"\n\n## Fitting Classification Trees\n\n###\nlibrary(tree)\n###\nlibrary(ISLR2)\nattach(Carseats)\nHigh <- factor(ifelse(Sales <= 8, \"No\", \"Yes\"))\n###\nCarseats <- data.frame(Carseats, High)\n###\ntree.carseats <- tree(High ~ . - Sales, Carseats)\n###\nsummary(tree.carseats)\n#> \n#> Classification tree:\n#> tree(formula = High ~ . - Sales, data = Carseats)\n#> Variables actually used in tree construction:\n#> [1] \"ShelveLoc\"   \"Price\"       \"Income\"      \"CompPrice\"  \n#> [5] \"Population\"  \"Advertising\" \"Age\"         \"US\"         \n#> Number of terminal nodes:  27 \n#> Residual mean deviance:  0.4575 = 170.7 / 373 \n#> Misclassification error rate: 0.09 = 36 / 400\n###\nplot(tree.carseats)\ntext(tree.carseats, pretty = 0)\n###\ntree.carseats\n#> node), split, n, deviance, yval, (yprob)\n#>       * denotes terminal node\n#> \n#>   1) root 400 541.500 No ( 0.59000 0.41000 )  \n#>     2) ShelveLoc: Bad,Medium 315 390.600 No ( 0.68889 0.31111 )  \n#>       4) Price < 92.5 46  56.530 Yes ( 0.30435 0.69565 )  \n#>         8) Income < 57 10  12.220 No ( 0.70000 0.30000 )  \n#>          16) CompPrice < 110.5 5   0.000 No ( 1.00000 0.00000 ) *\n#>          17) CompPrice > 110.5 5   6.730 Yes ( 0.40000 0.60000 ) *\n#>         9) Income > 57 36  35.470 Yes ( 0.19444 0.80556 )  \n#>          18) Population < 207.5 16  21.170 Yes ( 0.37500 0.62500 ) *\n#>          19) Population > 207.5 20   7.941 Yes ( 0.05000 0.95000 ) *\n#>       5) Price > 92.5 269 299.800 No ( 0.75465 0.24535 )  \n#>        10) Advertising < 13.5 224 213.200 No ( 0.81696 0.18304 )  \n#>          20) CompPrice < 124.5 96  44.890 No ( 0.93750 0.06250 )  \n#>            40) Price < 106.5 38  33.150 No ( 0.84211 0.15789 )  \n#>              80) Population < 177 12  16.300 No ( 0.58333 0.41667 )  \n#>               160) Income < 60.5 6   0.000 No ( 1.00000 0.00000 ) *\n#>               161) Income > 60.5 6   5.407 Yes ( 0.16667 0.83333 ) *\n#>              81) Population > 177 26   8.477 No ( 0.96154 0.03846 ) *\n#>            41) Price > 106.5 58   0.000 No ( 1.00000 0.00000 ) *\n#>          21) CompPrice > 124.5 128 150.200 No ( 0.72656 0.27344 )  \n#>            42) Price < 122.5 51  70.680 Yes ( 0.49020 0.50980 )  \n#>              84) ShelveLoc: Bad 11   6.702 No ( 0.90909 0.09091 ) *\n#>              85) ShelveLoc: Medium 40  52.930 Yes ( 0.37500 0.62500 )  \n#>               170) Price < 109.5 16   7.481 Yes ( 0.06250 0.93750 ) *\n#>               171) Price > 109.5 24  32.600 No ( 0.58333 0.41667 )  \n#>                 342) Age < 49.5 13  16.050 Yes ( 0.30769 0.69231 ) *\n#>                 343) Age > 49.5 11   6.702 No ( 0.90909 0.09091 ) *\n#>            43) Price > 122.5 77  55.540 No ( 0.88312 0.11688 )  \n#>              86) CompPrice < 147.5 58  17.400 No ( 0.96552 0.03448 ) *\n#>              87) CompPrice > 147.5 19  25.010 No ( 0.63158 0.36842 )  \n#>               174) Price < 147 12  16.300 Yes ( 0.41667 0.58333 )  \n#>                 348) CompPrice < 152.5 7   5.742 Yes ( 0.14286 0.85714 ) *\n#>                 349) CompPrice > 152.5 5   5.004 No ( 0.80000 0.20000 ) *\n#>               175) Price > 147 7   0.000 No ( 1.00000 0.00000 ) *\n#>        11) Advertising > 13.5 45  61.830 Yes ( 0.44444 0.55556 )  \n#>          22) Age < 54.5 25  25.020 Yes ( 0.20000 0.80000 )  \n#>            44) CompPrice < 130.5 14  18.250 Yes ( 0.35714 0.64286 )  \n#>              88) Income < 100 9  12.370 No ( 0.55556 0.44444 ) *\n#>              89) Income > 100 5   0.000 Yes ( 0.00000 1.00000 ) *\n#>            45) CompPrice > 130.5 11   0.000 Yes ( 0.00000 1.00000 ) *\n#>          23) Age > 54.5 20  22.490 No ( 0.75000 0.25000 )  \n#>            46) CompPrice < 122.5 10   0.000 No ( 1.00000 0.00000 ) *\n#>            47) CompPrice > 122.5 10  13.860 No ( 0.50000 0.50000 )  \n#>              94) Price < 125 5   0.000 Yes ( 0.00000 1.00000 ) *\n#>              95) Price > 125 5   0.000 No ( 1.00000 0.00000 ) *\n#>     3) ShelveLoc: Good 85  90.330 Yes ( 0.22353 0.77647 )  \n#>       6) Price < 135 68  49.260 Yes ( 0.11765 0.88235 )  \n#>        12) US: No 17  22.070 Yes ( 0.35294 0.64706 )  \n#>          24) Price < 109 8   0.000 Yes ( 0.00000 1.00000 ) *\n#>          25) Price > 109 9  11.460 No ( 0.66667 0.33333 ) *\n#>        13) US: Yes 51  16.880 Yes ( 0.03922 0.96078 ) *\n#>       7) Price > 135 17  22.070 No ( 0.64706 0.35294 )  \n#>        14) Income < 46 6   0.000 No ( 1.00000 0.00000 ) *\n#>        15) Income > 46 11  15.160 Yes ( 0.45455 0.54545 ) *\n###\nset.seed(2)\ntrain <- sample(1:nrow(Carseats), 200)\nCarseats.test <- Carseats[-train, ]\nHigh.test <- High[-train]\ntree.carseats <- tree(High ~ . - Sales, Carseats,\n                      subset = train)\ntree.pred <- predict(tree.carseats, Carseats.test,\n                     type = \"class\")\ntable(tree.pred, High.test)\n#>          High.test\n#> tree.pred  No Yes\n#>       No  104  33\n#>       Yes  13  50\n(104 + 50) / 200\n#> [1] 0.77\n###\nset.seed(7)\ncv.carseats <- cv.tree(tree.carseats, FUN = prune.misclass)\nnames(cv.carseats)\n#> [1] \"size\"   \"dev\"    \"k\"      \"method\"\ncv.carseats\n#> $size\n#> [1] 21 19 14  9  8  5  3  2  1\n#> \n#> $dev\n#> [1] 75 75 75 74 82 83 83 85 82\n#> \n#> $k\n#> [1] -Inf  0.0  1.0  1.4  2.0  3.0  4.0  9.0 18.0\n#> \n#> $method\n#> [1] \"misclass\"\n#> \n#> attr(,\"class\")\n#> [1] \"prune\"         \"tree.sequence\"\n###\npar(mfrow = c(1, 2))\nplot(cv.carseats$size, cv.carseats$dev, type = \"b\")\nplot(cv.carseats$k, cv.carseats$dev, type = \"b\")\n###\nprune.carseats <- prune.misclass(tree.carseats, best = 9)\nplot(prune.carseats)\ntext(prune.carseats, pretty = 0)\n###\ntree.pred <- predict(prune.carseats, Carseats.test,\n                     type = \"class\")\ntable(tree.pred, High.test)\n#>          High.test\n#> tree.pred No Yes\n#>       No  97  25\n#>       Yes 20  58\n(97 + 58) / 200\n#> [1] 0.775\n###\nprune.carseats <- prune.misclass(tree.carseats, best = 14)\nplot(prune.carseats)\ntext(prune.carseats, pretty = 0)\ntree.pred <- predict(prune.carseats, Carseats.test,\n                     type = \"class\")\ntable(tree.pred, High.test)\n#>          High.test\n#> tree.pred  No Yes\n#>       No  102  31\n#>       Yes  15  52\n(102 + 52) / 200\n#> [1] 0.77\n\n## Fitting Regression Trees\n\n###\nset.seed(1)\ntrain <- sample(1:nrow(Boston), nrow(Boston) / 2)\ntree.boston <- tree(medv ~ ., Boston, subset = train)\nsummary(tree.boston)\n#> \n#> Regression tree:\n#> tree(formula = medv ~ ., data = Boston, subset = train)\n#> Variables actually used in tree construction:\n#> [1] \"rm\"    \"lstat\" \"crim\"  \"age\"  \n#> Number of terminal nodes:  7 \n#> Residual mean deviance:  10.38 = 2555 / 246 \n#> Distribution of residuals:\n#>     Min.  1st Qu.   Median     Mean  3rd Qu.     Max. \n#> -10.1800  -1.7770  -0.1775   0.0000   1.9230  16.5800\n###\nplot(tree.boston)\ntext(tree.boston, pretty = 0)\n###\ncv.boston <- cv.tree(tree.boston)\nplot(cv.boston$size, cv.boston$dev, type = \"b\")\n###\nprune.boston <- prune.tree(tree.boston, best = 5)\nplot(prune.boston)\ntext(prune.boston, pretty = 0)\n###\nyhat <- predict(tree.boston, newdata = Boston[-train, ])\nboston.test <- Boston[-train, \"medv\"]\nplot(yhat, boston.test)\nabline(0, 1)\nmean((yhat - boston.test)^2)\n#> [1] 35.28688\n\n## Bagging and Random Forests\n\n###\nlibrary(randomForest)\n#> randomForest 4.7-1\n#> Type rfNews() to see new features/changes/bug fixes.\nset.seed(1)\nbag.boston <- randomForest(medv ~ ., data = Boston,\n                           subset = train, mtry = 12, importance = TRUE)\nbag.boston\n#> \n#> Call:\n#>  randomForest(formula = medv ~ ., data = Boston, mtry = 12, importance = TRUE,      subset = train) \n#>                Type of random forest: regression\n#>                      Number of trees: 500\n#> No. of variables tried at each split: 12\n#> \n#>           Mean of squared residuals: 11.40162\n#>                     % Var explained: 85.17\n###\nyhat.bag <- predict(bag.boston, newdata = Boston[-train, ])\nplot(yhat.bag, boston.test)\nabline(0, 1)\nmean((yhat.bag - boston.test)^2)\n#> [1] 23.41916\n###\nbag.boston <- randomForest(medv ~ ., data = Boston,\n                           subset = train, mtry = 12, ntree = 25)\nyhat.bag <- predict(bag.boston, newdata = Boston[-train, ])\nmean((yhat.bag - boston.test)^2)\n#> [1] 25.75055\n###\nset.seed(1)\nrf.boston <- randomForest(medv ~ ., data = Boston,\n                          subset = train, mtry = 6, importance = TRUE)\nyhat.rf <- predict(rf.boston, newdata = Boston[-train, ])\nmean((yhat.rf - boston.test)^2)\n#> [1] 20.06644\n###\nimportance(rf.boston)\n#>           %IncMSE IncNodePurity\n#> crim    19.435587    1070.42307\n#> zn       3.091630      82.19257\n#> indus    6.140529     590.09536\n#> chas     1.370310      36.70356\n#> nox     13.263466     859.97091\n#> rm      35.094741    8270.33906\n#> age     15.144821     634.31220\n#> dis      9.163776     684.87953\n#> rad      4.793720      83.18719\n#> tax      4.410714     292.20949\n#> ptratio  8.612780     902.20190\n#> lstat   28.725343    5813.04833\n###\nvarImpPlot(rf.boston)\n\n## Boosting\n\n###\nlibrary(gbm)\n#> Loaded gbm 2.1.8\nset.seed(1)\nboost.boston <- gbm(medv ~ ., data = Boston[train, ],\n                    distribution = \"gaussian\", n.trees = 5000,\n                    interaction.depth = 4)\n###\nsummary(boost.boston)\n#>             var     rel.inf\n#> rm           rm 44.48249588\n#> lstat     lstat 32.70281223\n#> crim       crim  4.85109954\n#> dis         dis  4.48693083\n#> nox         nox  3.75222394\n#> age         age  3.19769210\n#> ptratio ptratio  2.81354826\n#> tax         tax  1.54417603\n#> indus     indus  1.03384666\n#> rad         rad  0.87625748\n#> zn           zn  0.16220479\n#> chas       chas  0.09671228\n###\nplot(boost.boston, i = \"rm\")\nplot(boost.boston, i = \"lstat\")\n###\nyhat.boost <- predict(boost.boston,\n                      newdata = Boston[-train, ], n.trees = 5000)\nmean((yhat.boost - boston.test)^2)\n#> [1] 18.39057\n###\nboost.boston <- gbm(medv ~ ., data = Boston[train, ],\n                    distribution = \"gaussian\", n.trees = 5000,\n                    interaction.depth = 4, shrinkage = 0.2, verbose = F)\nyhat.boost <- predict(boost.boston,\n                      newdata = Boston[-train, ], n.trees = 5000)\nmean((yhat.boost - boston.test)^2)\n#> [1] 16.54778\n\n## Bayesian Additive Regression Trees\n\n###\nlibrary(BART)\n#> Loading required package: nlme\n#> Loading required package: nnet\n#> Loading required package: survival\nx <- Boston[, 1:12]\ny <- Boston[, \"medv\"]\nxtrain <- x[train, ]\nytrain <- y[train]\nxtest <- x[-train, ]\nytest <- y[-train]\nset.seed(1)\nbartfit <- gbart(xtrain, ytrain, x.test = xtest)\n#> *****Calling gbart: type=1\n#> *****Data:\n#> data:n,p,np: 253, 12, 253\n#> y1,yn: 0.213439, -5.486561\n#> x1,x[n*p]: 0.109590, 20.080000\n#> xp1,xp[np*p]: 0.027310, 7.880000\n#> *****Number of Trees: 200\n#> *****Number of Cut Points: 100 ... 100\n#> *****burn,nd,thin: 100,1000,1\n#> *****Prior:beta,alpha,tau,nu,lambda,offset: 2,0.95,0.795495,3,3.71636,21.7866\n#> *****sigma: 4.367914\n#> *****w (weights): 1.000000 ... 1.000000\n#> *****Dirichlet:sparse,theta,omega,a,b,rho,augment: 0,0,1,0.5,1,12,0\n#> *****printevery: 100\n#> \n#> MCMC\n#> done 0 (out of 1100)\n#> done 100 (out of 1100)\n#> done 200 (out of 1100)\n#> done 300 (out of 1100)\n#> done 400 (out of 1100)\n#> done 500 (out of 1100)\n#> done 600 (out of 1100)\n#> done 700 (out of 1100)\n#> done 800 (out of 1100)\n#> done 900 (out of 1100)\n#> done 1000 (out of 1100)\n#> time: 4s\n#> trcnt,tecnt: 1000,1000\n###\nyhat.bart <- bartfit$yhat.test.mean\nmean((ytest - yhat.bart)^2)\n#> [1] 15.94718\n###\nord <- order(bartfit$varcount.mean, decreasing = T)\nbartfit$varcount.mean[ord]\n#>     nox   lstat     tax     rad      rm   indus    chas \n#>  22.952  21.329  21.250  20.781  19.890  19.825  19.051 \n#> ptratio     age      zn     dis    crim \n#>  18.976  18.274  15.952  14.457  11.007\n###"},{"path":"trees-9-the-oj-dataset.html","id":"trees-9-the-oj-dataset","chapter":"17 Trees #9 The OJ Dataset","heading":"17 Trees #9 The OJ Dataset","text":"problem involves OJ data set part ISLR package.train/test SplitQ: Create training set containing random sample 800 observations, test set containing remaining observations.: Since first time seeing dataset, quick overview: OJ dataset contains 1,070 purchases two brands orange juice (‘Citrus Hill’ ‘Minute Maid’), captured values Purchase variable (CH MM). remaining 17 predictors characteristics customer, product, store, etc. Throughout question basically tasked predicting orange juice customer purchased based statistics.create train test .Classification TreeQ: Fit tree training data, Purchase response variables predictors. Use summary() function produce summary statistics tree, describe results obtained. training error rate? many terminal nodes tree ?: classification tree 7 terminal nodes training error rate 18.38%.Despite 17 predictors dataset, three used splits. :tree() - Text InterpretationQ: Type name tree object order get detailed text output. Pick one terminal nodes, interpret information displayed.: print text output .Choosing node 11), terminal node marked *:First root node: 1) root 800 1064.00 CH ( 0.61750 0.38250 )means , root node, 800 observations, deviance 1064.00, overall prediction CH split 61.75% CH vs 38.25% MM.can see , root node, three splits take place produce terminal node labelled 11):Node 11) therefore subset purchases 0.142213 < LoyalCH < 0.5036 PriceDiff > 0.235. overall prediction CH, node seems quite impure 57.627% CH vs 42.373% MM.118 observations node, percentages know 68/118 CH 50/118 MM (demonstrated ).Based formula page 325 overall deviance classification tree \\[ (−2∑m∑knmklog(p^mk))\\] overall deviance sum m regions (terminal nodes). calculate can deviance node 11) using code :tree() reports number 160.80, testing nodes revealed ’s rounding result 4 significant figures.PlottingQ: Create plot tree, interpret results.:\nLoyalCH certainly important variable (top 3 nodes split variable), followed PriceDiff DiscCH. can see node 11) third terminal node (left → right).LoyalCH ranges 0 1, first split sends less loyal Citrus Hill (CH) orange juice left loyal right:plot(tree_model)\ntext(tree_model, pretty = 0, cex = 0.7)scored lowest Citrus Hill loyalty (LoyalCH < 0.142213) predicted buy Minute Maid (MM), isn’t surprising. slightly loyal CH (0.142213 < LoyalCH < 0.5036) still buy MM wasn’t much expensive (PriceDiff < 0.235), price difference large enough (CH much cheaper) end purchasing CH.far-right terminal node loyal CH (LoyalCH > 0.705699), unsurprising predicted purchase. slightly lower brand loyalty (0.5036 < LoyalCH < 0.705699) still purchase CH much cheaper (PriceDiff > 0.25), wasn’t sufficiently discounted (PriceDiff < 0.25 & DiscCH > 0.15). cases CH wasn’t much cheaper (PriceDiff < 0.25) wasn’t sufficiently discounted (DiscCH < 0.15), predicted purchase actually ended MM.much detailed explanation, summarized much higher level following way: people go brand loyal towards, edge cases (based discounts prices relative one another) can sway people usual brand loyalties.Test ErrorQ: Predict response test data, produce confusion matrix comparing test labels predicted test labels. test error rate?:confusion matrix unpruned regression tree:","code":"#> \n#> Attaching package: 'dplyr'\n#> The following objects are masked from 'package:stats':\n#> \n#>     filter, lag\n#> The following objects are masked from 'package:base':\n#> \n#>     intersect, setdiff, setequal, union\ndplyr::glimpse(OJ)\n#> Rows: 1,070\n#> Columns: 18\n#> $ Purchase       <fct> CH, CH, CH, MM, CH, CH, CH, CH, CH,…\n#> $ WeekofPurchase <dbl> 237, 239, 245, 227, 228, 230, 232, …\n#> $ StoreID        <dbl> 1, 1, 1, 1, 7, 7, 7, 7, 7, 7, 7, 7,…\n#> $ PriceCH        <dbl> 1.75, 1.75, 1.86, 1.69, 1.69, 1.69,…\n#> $ PriceMM        <dbl> 1.99, 1.99, 2.09, 1.69, 1.69, 1.99,…\n#> $ DiscCH         <dbl> 0.00, 0.00, 0.17, 0.00, 0.00, 0.00,…\n#> $ DiscMM         <dbl> 0.00, 0.30, 0.00, 0.00, 0.00, 0.00,…\n#> $ SpecialCH      <dbl> 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0,…\n#> $ SpecialMM      <dbl> 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0,…\n#> $ LoyalCH        <dbl> 0.500000, 0.600000, 0.680000, 0.400…\n#> $ SalePriceMM    <dbl> 1.99, 1.69, 2.09, 1.69, 1.69, 1.99,…\n#> $ SalePriceCH    <dbl> 1.75, 1.75, 1.69, 1.69, 1.69, 1.69,…\n#> $ PriceDiff      <dbl> 0.24, -0.06, 0.40, 0.00, 0.00, 0.30…\n#> $ Store7         <fct> No, No, No, No, Yes, Yes, Yes, Yes,…\n#> $ PctDiscMM      <dbl> 0.000000, 0.150754, 0.000000, 0.000…\n#> $ PctDiscCH      <dbl> 0.000000, 0.000000, 0.091398, 0.000…\n#> $ ListPriceDiff  <dbl> 0.24, 0.24, 0.23, 0.00, 0.00, 0.30,…\n#> $ STORE          <dbl> 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0,…\nset.seed(5)\ntrain_index <- sample(1:nrow(OJ), 800)\ntrain <- OJ[train_index, ]\ntest <- OJ[-train_index, ]\ntree_model <- tree(Purchase ~ ., train)\nsummary(tree_model)\n#> \n#> Classification tree:\n#> tree(formula = Purchase ~ ., data = train)\n#> Variables actually used in tree construction:\n#> [1] \"LoyalCH\"       \"PriceDiff\"     \"ListPriceDiff\"\n#> Number of terminal nodes:  9 \n#> Residual mean deviance:  0.7347 = 581.1 / 791 \n#> Misclassification error rate: 0.1662 = 133 / 800LoyalCH - Customer brand loyalty for CH\nPriceDiff - Sale price of MM less sale price of CH\nDiscCH - Discount offered for CH\ntree_model\n#> node), split, n, deviance, yval, (yprob)\n#>       * denotes terminal node\n#> \n#>  1) root 800 1068.00 CH ( 0.61250 0.38750 )  \n#>    2) LoyalCH < 0.5036 346  412.40 MM ( 0.28324 0.71676 )  \n#>      4) LoyalCH < 0.280875 164  125.50 MM ( 0.12805 0.87195 )  \n#>        8) LoyalCH < 0.0356415 56   10.03 MM ( 0.01786 0.98214 ) *\n#>        9) LoyalCH > 0.0356415 108  103.50 MM ( 0.18519 0.81481 ) *\n#>      5) LoyalCH > 0.280875 182  248.00 MM ( 0.42308 0.57692 )  \n#>       10) PriceDiff < 0.05 71   67.60 MM ( 0.18310 0.81690 ) *\n#>       11) PriceDiff > 0.05 111  151.30 CH ( 0.57658 0.42342 ) *\n#>    3) LoyalCH > 0.5036 454  362.00 CH ( 0.86344 0.13656 )  \n#>      6) PriceDiff < -0.39 31   40.32 MM ( 0.35484 0.64516 )  \n#>       12) LoyalCH < 0.638841 10    0.00 MM ( 0.00000 1.00000 ) *\n#>       13) LoyalCH > 0.638841 21   29.06 CH ( 0.52381 0.47619 ) *\n#>      7) PriceDiff > -0.39 423  273.70 CH ( 0.90071 0.09929 )  \n#>       14) LoyalCH < 0.705326 135  143.00 CH ( 0.77778 0.22222 )  \n#>         28) ListPriceDiff < 0.255 67   89.49 CH ( 0.61194 0.38806 ) *\n#>         29) ListPriceDiff > 0.255 68   30.43 CH ( 0.94118 0.05882 ) *\n#>       15) LoyalCH > 0.705326 288   99.77 CH ( 0.95833 0.04167 ) *A split at LoyalCH = 0.5036\nA split at LoyalCH = 0.142213\nA split at PriceDiff = 0.235\n\n 1) root 800 1064.00 CH ( 0.61750 0.38250 )  \n   2) LoyalCH < 0.5036 354  435.50 MM ( 0.30508 0.69492 )  \n     4) LoyalCH < 0.142213 100   45.39 MM ( 0.06000 0.94000 ) *\n     5) LoyalCH > 0.142213 254  342.20 MM ( 0.40157 0.59843 )  \n      10) PriceDiff < 0.235 136  153.00 MM ( 0.25000 0.75000 ) *\n      11) PriceDiff > 0.235 118  160.80 CH ( 0.57627 0.42373 ) *\ntrain %>%\n  filter(LoyalCH < 0.5036, \n         LoyalCH > 0.142213, \n         PriceDiff > 0.235) %>%\n  select(Purchase) %>% \n  table()\n#> Purchase\n#> CH MM \n#> 57 54\n-2 * (68 * log(68/118) + 50 * log(50/118))\n#> [1] 160.8262\ntest_pred <- predict(tree_model, test, type = \"class\")\ntable(test_pred, test_actual = test$Purchase)\n#>          test_actual\n#> test_pred  CH  MM\n#>        CH 148  32\n#>        MM  15  75"},{"path":"trees-9-the-oj-dataset.html","id":"test_actual","chapter":"17 Trees #9 The OJ Dataset","heading":"17.1 test_actual","text":"","code":""},{"path":"trees-9-the-oj-dataset.html","id":"test_pred-ch-mm","chapter":"17 Trees #9 The OJ Dataset","heading":"17.2 test_pred CH MM","text":"","code":""},{"path":"trees-9-the-oj-dataset.html","id":"ch-125-32","chapter":"17 Trees #9 The OJ Dataset","heading":"17.3 CH 125 32","text":"","code":""},{"path":"trees-9-the-oj-dataset.html","id":"mm-34-79","chapter":"17 Trees #9 The OJ Dataset","heading":"17.4 MM 34 79","text":"test error rate corresponding :1 - mean(test_pred == test$Purchase)","code":""},{"path":"trees-9-the-oj-dataset.html","id":"section","chapter":"17 Trees #9 The OJ Dataset","heading":"17.5 [1] 0.2444444","text":"CH common orange juice train , comparison, baseline classifier (predicted CH observations test) following error rate:1 - mean(test$Purchase == “CH”)","code":""},{"path":"trees-9-the-oj-dataset.html","id":"section-1","chapter":"17 Trees #9 The OJ Dataset","heading":"17.6 [1] 0.4111111","text":"Cost-Complexity PruningQ: Apply cv.tree() function training set order determine optimal tree size.:Since goal appears low test error, specify FUN = prune.misclass. indicates want classification error rate guide cross-validation pruning process, rather default cv.tree() function, deviance.CV Error PlotQ: Produce plot tree size x-axis cross-validated classification error rate y-axis.:plot . Note cv_tree_model\\(size number terminal nodes (1 means just root node splits), cv_tree_model\\)dev gives total number errors made --fold predictions cross-validation (specified FUN = prune.misclass - omitting mean reports deviance). can obtain cross-validation error rate.Best Tree - CV ErrorQ: tree size corresponds lowest cross-validated classification error rate?:sequence trees generated, trees sizes 4 7 cross-validation error. makes sense select parsimonious model 4 terminal nodes.Best Tree - SelectingQ: Produce pruned tree corresponding optimal tree size obtained using cross-validation. cross-validation lead selection pruned tree, create pruned tree five terminal nodes.:produce tree 4 terminal nodes. Interestingly cross-validation error full tree 7 terminal nodes, split LoyalCH achieve . added benefit simplifying interpretation part (d).Training Error ComparisonQ: Compare training error rates pruned unpruned trees. higher?:\ntraining error unpruned tree (7 terminal nodes):pruned tree (4 terminal nodes):training error pruned tree higher. isn’t surprising - expect training error tree monotonically decrease flexibility (number splits) increases.Test Error ComparisonQ: Compare test error rates pruned unpruned trees. higher?:test error unpruned tree:pruned tree:Now order reversed error higher unpruned tree.interesting cross-validation errors fact equal test error noticeably lower simpler tree. lot probably comes random variability working small dataset; using different random state CV folds train/test split likely change results (particularly decision trees high-variance approaches).","code":"\nset.seed(2)\ncv_tree_model <- cv.tree(tree_model, K = 10, FUN = prune.misclass)\ncv_tree_model\n#> $size\n#> [1] 9 6 5 3 2 1\n#> \n#> $dev\n#> [1] 149 149 149 173 172 310\n#> \n#> $k\n#> [1]  -Inf   0.0   1.0   8.5   9.0 150.0\n#> \n#> $method\n#> [1] \"misclass\"\n#> \n#> attr(,\"class\")\n#> [1] \"prune\"         \"tree.sequence\"\ndata.frame(size = cv_tree_model$size, CV_Error = cv_tree_model$dev / nrow(train)) %>%\n  mutate(min_CV_Error = as.numeric(min(CV_Error) == CV_Error)) %>%\n  ggplot(aes(x = size, y = CV_Error)) +\n  geom_line(col = \"grey55\") +\n  geom_point(size = 2, aes(col = factor(min_CV_Error))) +\n  scale_x_continuous(breaks = seq(1, 7), minor_breaks = NULL) +\n  scale_y_continuous(labels = scales::percent_format()) +\n  scale_color_manual(values = c(\"deepskyblue3\", \"green\")) +\n  theme(legend.position = \"none\") +\n  labs(title = \"OJ Dataset - Classification Tree\",\n       subtitle = \"Selecting tree 'size' (# of terminal nodes) using cross-validation\",\n       x = \"Tree Size\",\n       y = \"CV Error\")\npruned_tree_model <- prune.tree(tree_model, best = 4)\npruned_tree_model\n#> node), split, n, deviance, yval, (yprob)\n#>       * denotes terminal node\n#> \n#> 1) root 800 1068.00 CH ( 0.61250 0.38750 )  \n#>   2) LoyalCH < 0.5036 346  412.40 MM ( 0.28324 0.71676 )  \n#>     4) LoyalCH < 0.280875 164  125.50 MM ( 0.12805 0.87195 ) *\n#>     5) LoyalCH > 0.280875 182  248.00 MM ( 0.42308 0.57692 ) *\n#>   3) LoyalCH > 0.5036 454  362.00 CH ( 0.86344 0.13656 )  \n#>     6) PriceDiff < -0.39 31   40.32 MM ( 0.35484 0.64516 ) *\n#>     7) PriceDiff > -0.39 423  273.70 CH ( 0.90071 0.09929 ) *\nmean(predict(tree_model, type = \"class\") != train$Purchase)\n#> [1] 0.16625\nmean(predict(pruned_tree_model, type = \"class\") != train$Purchase)\n#> [1] 0.18875\nmean(predict(tree_model, type = \"class\", newdata = test) != test$Purchase)\n#> [1] 0.1740741\nmean(predict(pruned_tree_model, type = \"class\", newdata = test) != test$Purchase)\n#> [1] 0.2"}]
