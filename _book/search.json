[{"path":"index.html","id":"about-this-course","chapter":"1 About this course","heading":"1 About this course","text":"website serves headquarters BADM 372 Applied Analytics.Content updated changes made semester, point told change schedule assignment, can come get updated version.Also, website benefited greatly lots free, readily available resources posted web leverage extensively. encourage review resources analytics journey. specifically use great frequency (say loud THANK authors!):R Data ScienceAn Introduction Statistical Learning Applications RData Science Boxstackoverflow.com, example","code":""},{"path":"syllabus.html","id":"syllabus","chapter":"2 Syllabus","heading":"2 Syllabus","text":"Instructor: Tobin TurnerOffice Hours: mutually convenient time arranged email e-mail: jtturner@presby.edu","code":""},{"path":"syllabus.html","id":"course-objectives-and-learning-outcomes","chapter":"2 Syllabus","heading":"2.1 Course Objectives and Learning Outcomes","text":"course designed introduce data science. Students apply statistical knowledge techniques business non-business contexts.end course students able :end course, able …gain insight datagain insight data, reproduciblygain insight data, reproducibly, using modern programming tools techniquesgain insight data, reproducibly collaboratively, using modern programming tools techniquesgain insight data, reproducibly (literate programming version control) collaboratively, using modern programming tools techniquescommunicate results effectivelyThis course focused understanding applying key business analytical concepts. Although text serves useful foundation concepts covered class, simple memorization material text sufficient. Class participation, discussion, application critical.","code":""},{"path":"syllabus.html","id":"text-and-resources","chapter":"2 Syllabus","heading":"2.2 Text and Resources","text":"course website (primary resource)R Data ScienceAn Introduction Statistical Learning Applications RData Science Boxstackoverflow.com, exampleOther free, publicly available datasets publications.","code":""},{"path":"syllabus.html","id":"performance-evaluation-grading","chapter":"2 Syllabus","heading":"2.3 Performance Evaluation (Grading)","text":"Labs, Quizzes Assignments - 40%Exam 1 - 20%Exam 2 - 20%Final Exam - 20%","code":""},{"path":"syllabus.html","id":"missed-examsassignments-zero","chapter":"2 Syllabus","heading":"2.3.1 Missed Exams/Assignments = ZERO","text":"Arrangements missed late assignments must made PRIOR exam due date student may receive ZERO grade exam assignment. See attendance quiz sections details.Missed Quizzes Assignments made later. present.","code":""},{"path":"syllabus.html","id":"exams","chapter":"2 Syllabus","heading":"2.3.2 Exams","text":"Exams cover assigned chapters textbook, assigned readings, lectures, class exercises, class discussions, videos, guest speakers. typically allocate time prior exam clearly identify body knowledge test cover answer questions format objectives exam.","code":""},{"path":"syllabus.html","id":"labsquizzes-dont-miss-class","chapter":"2 Syllabus","heading":"2.3.3 Labs/Quizzes – DON’T MISS CLASS","text":"average labs, quizzes assignments comprise Quizzes Assignments - 40% portion final gradeQuizzes Assignments designed prepare exams ensure stay course materialMissed Quizzes Assignments made later. present.Quizzes rule. LISTEN.\n- Missed Quizzes Assignments made later. present.","code":""},{"path":"syllabus.html","id":"final-average","chapter":"2 Syllabus","heading":"2.3.4 Final Average","text":"Final Average Grade\n90-100 \n88-89 B+\n82-87 B+\n80-81 B-\n78-79 C+\n72-77 C+\n70-71 C-\n60-69 D\n59 F\n90-100 A88-89 B+82-87 B+80-81 B-78-79 C+72-77 C+70-71 C-60-69 D59 F","code":""},{"path":"syllabus.html","id":"class-participation","chapter":"2 Syllabus","heading":"2.4 Class Participation:","text":"frequently give readings assignments complete prior next class meeting. expect fully engage material: answer questions, pose questions, provide insightful observations. Keep mind quality important component “participation.” Periodic cold calls take place. also put students “hot seat” occasion. class sessions, may select random group students lead us discussion debate. selection participants announced class begins, everyone expected prepare discussion. Reading assigned chapters articles best way prepare discussion. concerns called class, please see discuss. purpose “hot seat” stress embarrass students, encourage students actively engage material.","code":""},{"path":"syllabus.html","id":"phones","chapter":"2 Syllabus","heading":"2.5 Phones","text":"Phones allowed used class without instructor’s prior consent. need phone class please let know class. Unauthorized use electronic devices may result lowering grade dismissal class. mean .phone thing? mean .","code":""},{"path":"syllabus.html","id":"attendance","chapter":"2 Syllabus","heading":"2.6 Attendance","text":"expected regular punctual class attendance. Students responsible material missed homework assignments made. class missed, notes/homework obtained another student. 15 minutes late, class considered cancelled. 4 absences allowed semester. Exceeding absence policy may result receiving F course. professors roll official roll students present roll taken counted absent. student must miss exam, must work agreeable time instructor take test prior exam given. student misses test due emergency, student must inform instructor soon possible. special cases, instructor may allow student take make-exam.","code":""},{"path":"syllabus.html","id":"accommodations","chapter":"2 Syllabus","heading":"2.7 Accommodations","text":"Presbyterian College committed providing reasonable accommodations students documented disabilities. seeking academic accommodations Americans Disabilities Act, must register Academic Success Office, located 5th Avenue (beside Campus Police). receive accommodations, please obtain proper Accommodations Approval Form office, meet beginning semester discuss may deliver approved accommodations. especially encourage meet well advance actual accommodations provided, may feasible offer immediate accommodations without sufficient advance notice (case tests). can assure discussions remain confidential. Disability Services information located link http://bit.ly/PCdisabilityservicesAdditionally, student’s responsibility give instructor one week’s notice prior instance accommodation required.","code":""},{"path":"syllabus.html","id":"honor-code-and-plagiarism","chapter":"2 Syllabus","heading":"2.8 Honor Code and Plagiarism:","text":"assignments/exams must work. copying use unauthorized assistance treated violation PC’s Honor Code. unsure resources allowed, please ask. Please note text longer 7 words taken source must placed quotations cited. Also, summarizing source must also cited. Using source showing work violation plagiarism honor code.","code":""},{"path":"syllabus.html","id":"first-generation-version","chapter":"2 Syllabus","heading":"2.9 First-Generation Version:","text":"Presby First+ Advocate. support current first-generation students. Presbyterian College, first-generation students neither parent legal guardian graduated four-year higher education institution bachelor’s degree. first-generation college student, please contact . information support first-generation college students campus visit Presby First+ webpage.","code":""},{"path":"syllabus.html","id":"continuing-advocate-version","chapter":"2 Syllabus","heading":"2.10 Continuing Advocate Version","text":"Presby First+ Advocate. committed supporting first-generation students Presbyterian College. Presbyterian College, first-generation students neither parent legal guardian graduated four-year higher education institution bachelor’s degree. first-generation college student, please contact anytime visit office hours. information support first-generation college students campus visit Presby First+ webpage.","code":""},{"path":"schedule.html","id":"schedule","chapter":"3 Schedule","heading":"3 Schedule","text":"tentative schedule, change. best review often stay page may plan accordingly!","code":""},{"path":"schedule.html","id":"spring-2023","chapter":"3 Schedule","heading":"Spring 2023","text":"","code":""},{"path":"lab-1-excercises.html","id":"lab-1-excercises","chapter":"4 Lab 1 Excercises","heading":"4 Lab 1 Excercises","text":"Let’s make sure feel good BADM 371 material.open notes/internet/R4DS/etc., work must .Use starwars data (dplyr package) answer/:tallest individual? Shortest?many homeworlds ?homeworld individuals? Fewest? Average # idividuals per homeworld?Make plot individuals mass x axis height y axis.Put best fit line plot.biggest outlier dataset?Calculate BMI individuals. average BMI individuals?average BMI homeworld?homeworlds greatest percentage individuals BMI’s greater average found #8 ?many individuals missing data? variables missing data?","code":""},{"path":"lab-2-in-rmarkdown.html","id":"lab-2-in-rmarkdown","chapter":"5 Lab 2 in Rmarkdown","heading":"5 Lab 2 in Rmarkdown","text":"","code":""},{"path":"lab-2-in-rmarkdown.html","id":"r-markdown","chapter":"5 Lab 2 in Rmarkdown","heading":"5.1 R Markdown","text":"tallest individual? Shortest?many homeworlds ?homeworld individuals? Fewest? Average # individuals per homeworld?Make plot individuals mass x axis height y axis. Put best fit line plot. biggest outlier dataset?Make something like :","code":"\nlibrary(dplyr)#>    Min. 1st Qu.  Median    Mean 3rd Qu.    Max.    NA's \n#>    66.0   167.0   180.0   174.4   191.0   264.0       6#> # A tibble: 49 × 1\n#>    homeworld \n#>    <chr>     \n#>  1 Tatooine  \n#>  2 Naboo     \n#>  3 Alderaan  \n#>  4 Stewjon   \n#>  5 Eriadu    \n#>  6 Kashyyyk  \n#>  7 Corellia  \n#>  8 Rodia     \n#>  9 Nal Hutta \n#> 10 Bestine IV\n#> # … with 39 more rows#> # A tibble: 49 × 2\n#>    homeworld     n\n#>    <chr>     <int>\n#>  1 Naboo        11\n#>  2 Tatooine     10\n#>  3 <NA>         10\n#>  4 Alderaan      3\n#>  5 Coruscant     3\n#>  6 Kamino        3\n#>  7 Corellia      2\n#>  8 Kashyyyk      2\n#>  9 Mirial        2\n#> 10 Ryloth        2\n#> # … with 39 more rows\n#> # A tibble: 49 × 2\n#>    homeworld          n\n#>    <chr>          <int>\n#>  1 Aleen Minor        1\n#>  2 Bespin             1\n#>  3 Bestine IV         1\n#>  4 Cato Neimoidia     1\n#>  5 Cerea              1\n#>  6 Champala           1\n#>  7 Chandrila          1\n#>  8 Concord Dawn       1\n#>  9 Dathomir           1\n#> 10 Dorin              1\n#> # … with 39 more rows#> # A tibble: 1 × 3\n#>   name                   mass height\n#>   <chr>                 <dbl>  <int>\n#> 1 Jabba Desilijic Tiure  1358    175"},{"path":"lab-2-in-rmarkdown.html","id":"why-i-like-this-class","chapter":"5 Lab 2 in Rmarkdown","heading":"5.2 Why I like this class","text":"","code":""},{"path":"lab-2-in-rmarkdown.html","id":"its-awesome","chapter":"5 Lab 2 in Rmarkdown","heading":"5.2.1 Its awesome","text":"","code":""},{"path":"lab-2-in-rmarkdown.html","id":"really-awesome","chapter":"5 Lab 2 in Rmarkdown","heading":"5.2.1.1 Really Awesome","text":"","code":""},{"path":"lab-2-in-rmarkdown.html","id":"super-duper-awesome","chapter":"5 Lab 2 in Rmarkdown","heading":"5.2.1.1.1 Super Duper Awesome","text":"","code":""},{},{"path":"all-variables-considered.html","id":"all-variables-considered","chapter":"6 All Variables Considered:","heading":"6 All Variables Considered:","text":"","code":"#> # A tibble: 1 × 14\n#>    name height  mass hair_color skin_color eye_color\n#>   <int>  <int> <int>      <int>      <int>     <int>\n#> 1     0      6    28          5          0         0\n#> # … with 8 more variables: birth_year <int>, sex <int>,\n#> #   gender <int>, homeworld <int>, species <int>,\n#> #   films <int>, vehicles <int>, starships <int>"},{"path":"lab-2-ggplot-without-dsbox.html","id":"lab-2-ggplot-without-dsbox","chapter":"7 Lab 2 – ggplot without dsbox","heading":"7 Lab 2 – ggplot without dsbox","text":"","code":""},{"path":"lab-2-ggplot-without-dsbox.html","id":"excercises-using-the-data-sets-mpg-or-diamonds","chapter":"7 Lab 2 – ggplot without dsbox","heading":"7.1 Excercises using the data sets mpg or diamonds","text":"Create figures using data sets mpg diamonds needed:","code":""},{"path":"lab-2-ggplot-without-dsbox.html","id":"palmerpenguins","chapter":"7 Lab 2 – ggplot without dsbox","heading":"7.2 palmerpenguins","text":"palmerpenguins realtively new package CRAN, can install CRAN instead Github.Install like normal package. successful installation, can find two datasets attached package – penguins penguins_raw. can check help page (?penguins_raw ?penguins_raw) understand respective datasets.Please make well-labeled, meangingful plot show many missing variables variable dataset. results shoud look something like :Make plot showing count penguins species.Make plot showing count penguins species.Create plot illustrates relationship flipper_length_mm body_mass_g respect species.Create plot illustrates relationship flipper_length_mm body_mass_g respect species.Create plot illustrates relationship flipper_length_mm body_mass_g respect species island.Create plot illustrates relationship flipper_length_mm body_mass_g respect species island.Create plots using new/interesting geoms make sure plots meangiful, imprmfative labels, . possible examples:Create plots using new/interesting geoms make sure plots meangiful, imprmfative labels, . possible examples:","code":""},{"path":"functions.html","id":"functions","chapter":"8 Functions","heading":"8 Functions","text":"","code":""},{"path":"functions.html","id":"writing-functions","chapter":"8 Functions","heading":"8.1 Writing Functions","text":"","code":""},{"path":"functions.html","id":"fahrenheit-to-kelvin","chapter":"8 Functions","heading":"8.1.1 Fahrenheit to Kelvin","text":"\\(k = ((f - 32) * (5 / 9)) + 273.15\\)","code":"\n((32 - 32) * (5 / 9)) + 273.15\n#> [1] 273.15\n((212 - 32) * (5 / 9)) + 273.15\n#> [1] 373.15\n((-42 - 32) * (5 / 9)) + 273.15\n#> [1] 232.0389\nf_k <- function(f_temp) {\n    ((f_temp - 32) * (5 / 9)) + 273.15\n}\nf_k(32)\n#> [1] 273.15\nf_k(212)\n#> [1] 373.15\nf_k(-42)\n#> [1] 232.0389"},{"path":"functions.html","id":"kelvin-to-celsius","chapter":"8 Functions","heading":"8.1.2 Kelvin to Celsius","text":"","code":"\nk_c <- function(temp_k) {\n    temp_c <- temp_k - 273.15\n    return(temp_c)\n}\nk_c(0)\n#> [1] -273.15"},{"path":"functions.html","id":"fahrenheit-to-celsius","chapter":"8 Functions","heading":"8.1.3 Fahrenheit to Celsius","text":"","code":"\nf_c <- function(temp_f) {\n    temp_k <- f_k(temp_f)\n    temp_c <- k_c(temp_k)\n    return(temp_c)\n}\nf_c(32)\n#> [1] 0\nf_c(212)\n#> [1] 100"},{"path":"functions.html","id":"testing-functions","chapter":"8 Functions","heading":"8.2 Testing Functions","text":"","code":"\nlibrary(testthat)\ntestthat::expect_equal(f_c(32), 0)\ntestthat::expect_equal(f_c(212), 100)"},{"path":"functions.html","id":"exercise","chapter":"8 Functions","heading":"8.3 Exercise","text":"happens use NA, Inf, -Inf function?better names give functions wrote?name functions package?","code":""},{"path":"functions.html","id":"checking-values","chapter":"8 Functions","heading":"8.4 Checking values","text":"Calculating weighted meansIf expect lengths ,\ntest function","code":"\nmean_wt <- function(x, w) {\n  sum(x * w) / sum(w)\n}\nmean_wt(1:6, 1:6)\n#> [1] 4.333333\nmean_wt(1:6, 1:3)\n#> [1] 7.666667\nmean_wt <- function(x, w) {\n  if (length(x) != length(w)) {\n    stop(\"`x` and `w` should be the same length\")\n  }\n  sum(x * w) / sum(w)\n}\nmean_wt(1:6, 1:3)\n#> Error in mean_wt(1:6, 1:3): `x` and `w` should be the same length"},{"path":"functions.html","id":"dot-dot-dot","chapter":"8 Functions","heading":"8.5 dot-dot-dot …","text":"Use pass arguments another function inside.can also use force named arguments function.","code":"\nsum_3 <- function(x, y, z) {\n  return(x + y + z)\n}\nsum_3(1, 2, 3)\n#> [1] 6\nsum_3 <- function(x, y, ..., z) {\n  return(x + y + z)\n}\nsum_3(1, 2, z = 3)\n#> [1] 6\nsum_3(1, 2, z = 3)\n#> [1] 6"},{"path":"functions.html","id":"conditionals","chapter":"8 Functions","heading":"8.6 Conditionals","text":"","code":""},{"path":"functions.html","id":"if-statements","chapter":"8 Functions","heading":"8.7 if statements","text":"current function deal missing numbers","code":"\n# make a modification to this function\nk_c <- function(temp_k) {\n    if (temp_k < 0) {\n        warning('you passed in a negative Kelvin number')\n        # stop()\n        return(NA)\n    }\n    temp_c <- temp_k - 273.15\n    return(temp_c)\n}\nk_c(-9)\n#> Warning in k_c(-9): you passed in a negative Kelvin number\n#> [1] NA\nk_c(NA)Error in if (temp_k < 0) { : missing value where TRUE/FALSE needed\nk_c(0)\n#> [1] -273.15"},{"path":"functions.html","id":"if-else-statements","chapter":"8 Functions","heading":"8.8 If else statements","text":"current function deal missing numbers","code":"\nk_c <- function(temp_k) {\n    if (temp_k < 0) {\n        warning('you passed in a negative Kelvin number')\n        # stop()\n        return(NA)\n    } else {\n        temp_c <- temp_k - 273.15\n        return(temp_c)\n    }\n}\nk_c(-9)\n#> Warning in k_c(-9): you passed in a negative Kelvin number\n#> [1] NA\nk_c(NA)\nk_c(0)\n#> [1] -273.15"},{"path":"functions.html","id":"dealing-with-na","chapter":"8 Functions","heading":"8.9 Dealing with NA","text":"Re-write function work missing values.Note need make NA check first.use && || short-circuit boolean comparisons.\nalso guarantee value length 1L.\n== also vectorized, use identical() .equal().identical strict. Doesn’t corece types..equal ability set tolerances..equal: compare R objects x y testing ‘near equality’. different, comparison still made extent, report differences returned. use .equal directly expressions—either use isTRUE(.equal(….)) identical appropriate.","code":"\nk_c <- function(temp_k) {\n    if (is.na(temp_k)) {\n        return(NA)\n    } else if (temp_k < 0) {\n        warning('you passed in a negative Kelvin number')\n        # stop()\n        return(NA)\n    } else {\n        temp_c <- temp_k - 273.15\n        return(temp_c)\n    }\n}\nk_c(-9)\n#> Warning in k_c(-9): you passed in a negative Kelvin number\n#> [1] NA\nk_c(NA)\n#> [1] NA\nk_c(0)\n#> [1] -273.15\nidentical(0L, 0)\n#> [1] FALSE\nall.equal(0L, 0)\n#> [1] TRUE\nif (isTRUE(all.equal(0L, 0))) {print(\"Hello\")}\n#> [1] \"Hello\""},{"path":"functions.html","id":"fizzbuzz","chapter":"8 Functions","heading":"8.10 Fizzbuzz","text":"Check modulo 3 ","code":"\nfizzbuzz <- function(x) {\n  # these two lines check that x is a valid input\n  stopifnot(length(x) == 1)\n  stopifnot(is.numeric(x))\n  if (!(x %% 3) && !(x %% 5)) {\n    \"fizzbuzz\"\n  } else if (!(x %% 3)) {\n    \"fizz\"\n  } else if (!(x %% 5)) {\n    \"buzz\"\n  } else {\n    # ensure that the function returns a character vector\n    as.character(x)\n  }\n}\nfizzbuzz(6)\n#> [1] \"fizz\"\nfizzbuzz2 <- function(x) {\n  # these two lines check that x is a valid input\n  stopifnot(length(x) == 1)\n  stopifnot(is.numeric(x))\n  if (!(x %% 3)) {\n    if (!(x %% 5)) {\n      \"fizzbuzz\"\n    } else {\n      \"fizz\"\n    }\n  } else if (!(x %% 5)) {\n    \"buzz\"\n  } else {\n    # ensure that the function returns a character vector\n    as.character(x)\n  }\n}\nfizzbuzz(6)\n#> [1] \"fizz\""},{"path":"functions.html","id":"vectorized-conditionals","chapter":"8 Functions","heading":"8.10.1 Vectorized conditionals","text":"","code":"\nlibrary(dplyr)\n#> \n#> Attaching package: 'dplyr'\n#> The following object is masked from 'package:testthat':\n#> \n#>     matches\n#> The following objects are masked from 'package:stats':\n#> \n#>     filter, lag\n#> The following objects are masked from 'package:base':\n#> \n#>     intersect, setdiff, setequal, union\nfizzbuzz_vec <- function(x) {\n  dplyr::case_when(\n    !(x %% 3) & !(x %% 5) ~ \"fizzbuzz\",\n    !(x %% 3) ~ \"fizz\",\n    !(x %% 5) ~ \"buzz\",\n    TRUE ~ as.character(x)\n  )\n}\nfizzbuzz(1:10)\n#> Error in fizzbuzz(1:10): length(x) == 1 is not TRUE\nfizzbuzz_vec(1:10)\n#>  [1] \"1\"    \"2\"    \"fizz\" \"4\"    \"buzz\" \"fizz\" \"7\"    \"8\"   \n#>  [9] \"fizz\" \"buzz\""},{"path":"functions.html","id":"multiple-conditions","chapter":"8 Functions","heading":"8.10.2 Multiple conditions","text":"","code":"\nif (this) {\n  # do that\n} else if (that) {\n  # do something else\n} else {\n  # \n}"},{"path":"linear-regression.html","id":"linear-regression","chapter":"9 Linear Regression","heading":"9 Linear Regression","text":"resource ISLR chapter 3: linear regression.","code":""},{"path":"linear-regression.html","id":"exercises","chapter":"9 Linear Regression","heading":"9.1 Exercises","text":"Make sure can define terms outloud, words, make sense someone else (?). Actually practice saying defining terms outloud answers make sense:least squares approachconfidence intervalp-valueR2Adjusted R2qualitative predictorcollinearityKNNResidual standard errorF-statisticExplain point Figure 3.1In m1, , variables significant predictors Balance? know?“good” model created m1? know?Add Credit variables model m1. Can find two variables extremely high collinearity? ? know high collinearity? make sense given variables mean?Based model , predict balance individual 40, income $100,000, 16 years education, Asian student?Interpret model output, especially coefficients Income:Education   0.3149:’s going ?","code":"\nlibrary(\"ISLR\")\ndata(Credit)\nattach(Credit)\nhead(Credit)\n#>   ID  Income Limit Rating Cards Age Education Gender\n#> 1  1  14.891  3606    283     2  34        11   Male\n#> 2  2 106.025  6645    483     3  82        15 Female\n#> 3  3 104.593  7075    514     4  71        11   Male\n#> 4  4 148.924  9504    681     3  36        11 Female\n#> 5  5  55.882  4897    357     2  68        16   Male\n#> 6  6  80.180  8047    569     4  77        10   Male\n#>   Student Married Ethnicity Balance\n#> 1      No     Yes Caucasian     333\n#> 2     Yes     Yes     Asian     903\n#> 3      No      No     Asian     580\n#> 4      No      No     Asian     964\n#> 5      No     Yes Caucasian     331\n#> 6      No      No Caucasian    1151\nm1 <- lm(Balance ~ Age + Income + Education, data = Credit)\nsummary(m1)\n#> \n#> Call:\n#> lm(formula = Balance ~ Age + Income + Education, data = Credit)\n#> \n#> Residuals:\n#>     Min      1Q  Median      3Q     Max \n#> -867.14 -343.14  -49.44  316.55 1080.56 \n#> \n#> Coefficients:\n#>             Estimate Std. Error t value Pr(>|t|)    \n#> (Intercept) 348.8115   112.6895   3.095  0.00211 ** \n#> Age          -2.1863     1.2004  -1.821  0.06930 .  \n#> Income        6.2380     0.5877  10.614  < 2e-16 ***\n#> Education     0.8058     6.5254   0.123  0.90179    \n#> ---\n#> Signif. codes:  \n#> 0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n#> \n#> Residual standard error: 407.2 on 396 degrees of freedom\n#> Multiple R-squared:  0.2215, Adjusted R-squared:  0.2156 \n#> F-statistic: 37.56 on 3 and 396 DF,  p-value: < 2.2e-16\nm2 <- lm(Balance ~ Age + Income + Education + Ethnicity + Student, data = Credit)\nsummary(m2)\n#> \n#> Call:\n#> lm(formula = Balance ~ Age + Income + Education + Ethnicity + \n#>     Student, data = Credit)\n#> \n#> Residuals:\n#>     Min      1Q  Median      3Q     Max \n#> -818.77 -322.14  -54.52  315.67  781.45 \n#> \n#> Coefficients:\n#>                    Estimate Std. Error t value Pr(>|t|)    \n#> (Intercept)        336.6241   115.6311   2.911  0.00381 ** \n#> Age                 -1.9756     1.1595  -1.704  0.08922 .  \n#> Income               6.1491     0.5666  10.853  < 2e-16 ***\n#> Education           -1.7606     6.3060  -0.279  0.78024    \n#> EthnicityAsian     -14.2547    55.5240  -0.257  0.79752    \n#> EthnicityCaucasian   8.8839    48.3276   0.184  0.85424    \n#> StudentYes         382.0498    65.6854   5.816 1.25e-08 ***\n#> ---\n#> Signif. codes:  \n#> 0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n#> \n#> Residual standard error: 392.2 on 393 degrees of freedom\n#> Multiple R-squared:  0.2833, Adjusted R-squared:  0.2723 \n#> F-statistic: 25.89 on 6 and 393 DF,  p-value: < 2.2e-16\nm3 <- lm(Balance ~ Income*Education, data = Credit)\nsummary(m3)\n#> \n#> Call:\n#> lm(formula = Balance ~ Income * Education, data = Credit)\n#> \n#> Residuals:\n#>     Min      1Q  Median      3Q     Max \n#> -858.07 -349.99  -56.12  304.51 1083.93 \n#> \n#> Coefficients:\n#>                  Estimate Std. Error t value Pr(>|t|)   \n#> (Intercept)      435.4599   147.1000   2.960  0.00326 **\n#> Income             1.8168     2.4727   0.735  0.46294   \n#> Education        -13.9887    10.5931  -1.321  0.18741   \n#> Income:Education   0.3149     0.1788   1.761  0.07902 . \n#> ---\n#> Signif. codes:  \n#> 0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n#> \n#> Residual standard error: 407.3 on 396 degrees of freedom\n#> Multiple R-squared:  0.2211, Adjusted R-squared:  0.2152 \n#> F-statistic: 37.47 on 3 and 396 DF,  p-value: < 2.2e-16\noptions(scipen=999)\noptions(digits=3)\nlibrary(ISLR)\ndata(\"Credit\")\nattach(Credit)\n#> The following objects are masked from Credit (pos = 3):\n#> \n#>     Age, Balance, Cards, Education, Ethnicity,\n#>     Gender, ID, Income, Limit, Married, Rating,\n#>     Student\n?Credit\n#> starting httpd help server ...\n#>  done\nm1 <- lm(Balance~Income+Education)\nsummary(m1)\n#> \n#> Call:\n#> lm(formula = Balance ~ Income + Education)\n#> \n#> Residuals:\n#>    Min     1Q Median     3Q    Max \n#> -806.2 -349.7  -53.4  330.4 1103.4 \n#> \n#> Coefficients:\n#>             Estimate Std. Error t value            Pr(>|t|)\n#> (Intercept)  236.975     94.767    2.50               0.013\n#> Income         6.050      0.580   10.43 <0.0000000000000002\n#> Education      0.703      6.544    0.11               0.914\n#>                \n#> (Intercept) *  \n#> Income      ***\n#> Education      \n#> ---\n#> Signif. codes:  \n#> 0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n#> \n#> Residual standard error: 408 on 397 degrees of freedom\n#> Multiple R-squared:  0.215,  Adjusted R-squared:  0.211 \n#> F-statistic: 54.4 on 2 and 397 DF,  p-value: <0.0000000000000002\n236.975+(6.050 *100)+(0.703*12)\n#> [1] 850\npredict(m1, newdata = list(Income = 100, Education = 12))\n#>   1 \n#> 850\n\nm2 <- lm(Balance~Income*Education)\nsummary(m2)\n#> \n#> Call:\n#> lm(formula = Balance ~ Income * Education)\n#> \n#> Residuals:\n#>    Min     1Q Median     3Q    Max \n#> -858.1 -350.0  -56.1  304.5 1083.9 \n#> \n#> Coefficients:\n#>                  Estimate Std. Error t value Pr(>|t|)   \n#> (Intercept)       435.460    147.100    2.96   0.0033 **\n#> Income              1.817      2.473    0.73   0.4629   \n#> Education         -13.989     10.593   -1.32   0.1874   \n#> Income:Education    0.315      0.179    1.76   0.0790 . \n#> ---\n#> Signif. codes:  \n#> 0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n#> \n#> Residual standard error: 407 on 396 degrees of freedom\n#> Multiple R-squared:  0.221,  Adjusted R-squared:  0.215 \n#> F-statistic: 37.5 on 3 and 396 DF,  p-value: <0.0000000000000002\n\n435.460+(1.817*100)+(-13.989*12)+(0.315*100*12)\n#> [1] 827\npredict(m2, newdata = list(Income = 100, Education = 12))\n#>   1 \n#> 827"},{"path":"logistic-regression.html","id":"logistic-regression","chapter":"10 Logistic Regression","heading":"10 Logistic Regression","text":"resource ISLR Chapter 4: Classification.","code":""},{"path":"tree-based-methods.html","id":"tree-based-methods","chapter":"11 Tree-Based Methods","heading":"11 Tree-Based Methods","text":"resource ISLR Chapter 8: Tree-Based Methods.Make sure can exlain terms/ideas/figures outloud, words, make sense someone else (?). Actually practice exlaining terms/ideas/figures outloud answers make sense:Regression vs. classification treesUnderstand Figure 8.1, Figure 8.2, Algorithm 8.1, Figure 8.3, Figure 8.4, Figure 8.5, Figure 8.6, Figure 8.7Be able explain +’s -’s trees (see section 8.1.4)Understand “combining large number trees\ncan often result dramatic improvements prediction accuracy, expense loss interpretation.”top-, greedy approach (aka recursive binary splitting)tree pruning subtreescost complexity pruning (aka weakest link pruning)baggingrandom forestsboostingBayesian additive regression trees","code":""},{"path":"chapter-8-lab-decision-trees.html","id":"chapter-8-lab-decision-trees","chapter":"12 Chapter 8 Lab: Decision Trees","heading":"12 Chapter 8 Lab: Decision Trees","text":"**Check Video: StatsLearning Lect10 R trees 111213Or Click : !!!","code":"\n\n## Fitting Classification Trees\n\n###\nlibrary(tree)\n###\nlibrary(ISLR2)\nattach(Carseats)\nHigh <- factor(ifelse(Sales <= 8, \"No\", \"Yes\"))\n###\nCarseats <- data.frame(Carseats, High)\n###\ntree.carseats <- tree(High ~ . - Sales, Carseats)\n###\nsummary(tree.carseats)\n#> \n#> Classification tree:\n#> tree(formula = High ~ . - Sales, data = Carseats)\n#> Variables actually used in tree construction:\n#> [1] \"ShelveLoc\"   \"Price\"       \"Income\"      \"CompPrice\"  \n#> [5] \"Population\"  \"Advertising\" \"Age\"         \"US\"         \n#> Number of terminal nodes:  27 \n#> Residual mean deviance:  0.4575 = 170.7 / 373 \n#> Misclassification error rate: 0.09 = 36 / 400\n###\nplot(tree.carseats)\ntext(tree.carseats, pretty = 0)\n###\ntree.carseats\n#> node), split, n, deviance, yval, (yprob)\n#>       * denotes terminal node\n#> \n#>   1) root 400 541.500 No ( 0.59000 0.41000 )  \n#>     2) ShelveLoc: Bad,Medium 315 390.600 No ( 0.68889 0.31111 )  \n#>       4) Price < 92.5 46  56.530 Yes ( 0.30435 0.69565 )  \n#>         8) Income < 57 10  12.220 No ( 0.70000 0.30000 )  \n#>          16) CompPrice < 110.5 5   0.000 No ( 1.00000 0.00000 ) *\n#>          17) CompPrice > 110.5 5   6.730 Yes ( 0.40000 0.60000 ) *\n#>         9) Income > 57 36  35.470 Yes ( 0.19444 0.80556 )  \n#>          18) Population < 207.5 16  21.170 Yes ( 0.37500 0.62500 ) *\n#>          19) Population > 207.5 20   7.941 Yes ( 0.05000 0.95000 ) *\n#>       5) Price > 92.5 269 299.800 No ( 0.75465 0.24535 )  \n#>        10) Advertising < 13.5 224 213.200 No ( 0.81696 0.18304 )  \n#>          20) CompPrice < 124.5 96  44.890 No ( 0.93750 0.06250 )  \n#>            40) Price < 106.5 38  33.150 No ( 0.84211 0.15789 )  \n#>              80) Population < 177 12  16.300 No ( 0.58333 0.41667 )  \n#>               160) Income < 60.5 6   0.000 No ( 1.00000 0.00000 ) *\n#>               161) Income > 60.5 6   5.407 Yes ( 0.16667 0.83333 ) *\n#>              81) Population > 177 26   8.477 No ( 0.96154 0.03846 ) *\n#>            41) Price > 106.5 58   0.000 No ( 1.00000 0.00000 ) *\n#>          21) CompPrice > 124.5 128 150.200 No ( 0.72656 0.27344 )  \n#>            42) Price < 122.5 51  70.680 Yes ( 0.49020 0.50980 )  \n#>              84) ShelveLoc: Bad 11   6.702 No ( 0.90909 0.09091 ) *\n#>              85) ShelveLoc: Medium 40  52.930 Yes ( 0.37500 0.62500 )  \n#>               170) Price < 109.5 16   7.481 Yes ( 0.06250 0.93750 ) *\n#>               171) Price > 109.5 24  32.600 No ( 0.58333 0.41667 )  \n#>                 342) Age < 49.5 13  16.050 Yes ( 0.30769 0.69231 ) *\n#>                 343) Age > 49.5 11   6.702 No ( 0.90909 0.09091 ) *\n#>            43) Price > 122.5 77  55.540 No ( 0.88312 0.11688 )  \n#>              86) CompPrice < 147.5 58  17.400 No ( 0.96552 0.03448 ) *\n#>              87) CompPrice > 147.5 19  25.010 No ( 0.63158 0.36842 )  \n#>               174) Price < 147 12  16.300 Yes ( 0.41667 0.58333 )  \n#>                 348) CompPrice < 152.5 7   5.742 Yes ( 0.14286 0.85714 ) *\n#>                 349) CompPrice > 152.5 5   5.004 No ( 0.80000 0.20000 ) *\n#>               175) Price > 147 7   0.000 No ( 1.00000 0.00000 ) *\n#>        11) Advertising > 13.5 45  61.830 Yes ( 0.44444 0.55556 )  \n#>          22) Age < 54.5 25  25.020 Yes ( 0.20000 0.80000 )  \n#>            44) CompPrice < 130.5 14  18.250 Yes ( 0.35714 0.64286 )  \n#>              88) Income < 100 9  12.370 No ( 0.55556 0.44444 ) *\n#>              89) Income > 100 5   0.000 Yes ( 0.00000 1.00000 ) *\n#>            45) CompPrice > 130.5 11   0.000 Yes ( 0.00000 1.00000 ) *\n#>          23) Age > 54.5 20  22.490 No ( 0.75000 0.25000 )  \n#>            46) CompPrice < 122.5 10   0.000 No ( 1.00000 0.00000 ) *\n#>            47) CompPrice > 122.5 10  13.860 No ( 0.50000 0.50000 )  \n#>              94) Price < 125 5   0.000 Yes ( 0.00000 1.00000 ) *\n#>              95) Price > 125 5   0.000 No ( 1.00000 0.00000 ) *\n#>     3) ShelveLoc: Good 85  90.330 Yes ( 0.22353 0.77647 )  \n#>       6) Price < 135 68  49.260 Yes ( 0.11765 0.88235 )  \n#>        12) US: No 17  22.070 Yes ( 0.35294 0.64706 )  \n#>          24) Price < 109 8   0.000 Yes ( 0.00000 1.00000 ) *\n#>          25) Price > 109 9  11.460 No ( 0.66667 0.33333 ) *\n#>        13) US: Yes 51  16.880 Yes ( 0.03922 0.96078 ) *\n#>       7) Price > 135 17  22.070 No ( 0.64706 0.35294 )  \n#>        14) Income < 46 6   0.000 No ( 1.00000 0.00000 ) *\n#>        15) Income > 46 11  15.160 Yes ( 0.45455 0.54545 ) *\n###\nset.seed(2)\ntrain <- sample(1:nrow(Carseats), 200)\nCarseats.test <- Carseats[-train, ]\nHigh.test <- High[-train]\ntree.carseats <- tree(High ~ . - Sales, Carseats,\n                      subset = train)\ntree.pred <- predict(tree.carseats, Carseats.test,\n                     type = \"class\")\ntable(tree.pred, High.test)\n#>          High.test\n#> tree.pred  No Yes\n#>       No  104  33\n#>       Yes  13  50\n(104 + 50) / 200\n#> [1] 0.77\n###\nset.seed(7)\ncv.carseats <- cv.tree(tree.carseats, FUN = prune.misclass)\nnames(cv.carseats)\n#> [1] \"size\"   \"dev\"    \"k\"      \"method\"\ncv.carseats\n#> $size\n#> [1] 21 19 14  9  8  5  3  2  1\n#> \n#> $dev\n#> [1] 75 75 75 74 82 83 83 85 82\n#> \n#> $k\n#> [1] -Inf  0.0  1.0  1.4  2.0  3.0  4.0  9.0 18.0\n#> \n#> $method\n#> [1] \"misclass\"\n#> \n#> attr(,\"class\")\n#> [1] \"prune\"         \"tree.sequence\"\n###\npar(mfrow = c(1, 2))\nplot(cv.carseats$size, cv.carseats$dev, type = \"b\")\nplot(cv.carseats$k, cv.carseats$dev, type = \"b\")\n###\nprune.carseats <- prune.misclass(tree.carseats, best = 9)\nplot(prune.carseats)\ntext(prune.carseats, pretty = 0)\n###\ntree.pred <- predict(prune.carseats, Carseats.test,\n                     type = \"class\")\ntable(tree.pred, High.test)\n#>          High.test\n#> tree.pred No Yes\n#>       No  97  25\n#>       Yes 20  58\n(97 + 58) / 200\n#> [1] 0.775\n###\nprune.carseats <- prune.misclass(tree.carseats, best = 14)\nplot(prune.carseats)\ntext(prune.carseats, pretty = 0)\ntree.pred <- predict(prune.carseats, Carseats.test,\n                     type = \"class\")\ntable(tree.pred, High.test)\n#>          High.test\n#> tree.pred  No Yes\n#>       No  102  31\n#>       Yes  15  52\n(102 + 52) / 200\n#> [1] 0.77\n\n## Fitting Regression Trees\n\n###\nset.seed(1)\ntrain <- sample(1:nrow(Boston), nrow(Boston) / 2)\ntree.boston <- tree(medv ~ ., Boston, subset = train)\nsummary(tree.boston)\n#> \n#> Regression tree:\n#> tree(formula = medv ~ ., data = Boston, subset = train)\n#> Variables actually used in tree construction:\n#> [1] \"rm\"    \"lstat\" \"crim\"  \"age\"  \n#> Number of terminal nodes:  7 \n#> Residual mean deviance:  10.38 = 2555 / 246 \n#> Distribution of residuals:\n#>     Min.  1st Qu.   Median     Mean  3rd Qu.     Max. \n#> -10.1800  -1.7770  -0.1775   0.0000   1.9230  16.5800\n###\nplot(tree.boston)\ntext(tree.boston, pretty = 0)\n###\ncv.boston <- cv.tree(tree.boston)\nplot(cv.boston$size, cv.boston$dev, type = \"b\")\n###\nprune.boston <- prune.tree(tree.boston, best = 5)\nplot(prune.boston)\ntext(prune.boston, pretty = 0)\n###\nyhat <- predict(tree.boston, newdata = Boston[-train, ])\nboston.test <- Boston[-train, \"medv\"]\nplot(yhat, boston.test)\nabline(0, 1)\nmean((yhat - boston.test)^2)\n#> [1] 35.28688\n\n## Bagging and Random Forests\n\n###\nlibrary(randomForest)\n#> randomForest 4.7-1\n#> Type rfNews() to see new features/changes/bug fixes.\nset.seed(1)\nbag.boston <- randomForest(medv ~ ., data = Boston,\n                           subset = train, mtry = 12, importance = TRUE)\nbag.boston\n#> \n#> Call:\n#>  randomForest(formula = medv ~ ., data = Boston, mtry = 12, importance = TRUE,      subset = train) \n#>                Type of random forest: regression\n#>                      Number of trees: 500\n#> No. of variables tried at each split: 12\n#> \n#>           Mean of squared residuals: 11.40162\n#>                     % Var explained: 85.17\n###\nyhat.bag <- predict(bag.boston, newdata = Boston[-train, ])\nplot(yhat.bag, boston.test)\nabline(0, 1)\nmean((yhat.bag - boston.test)^2)\n#> [1] 23.41916\n###\nbag.boston <- randomForest(medv ~ ., data = Boston,\n                           subset = train, mtry = 12, ntree = 25)\nyhat.bag <- predict(bag.boston, newdata = Boston[-train, ])\nmean((yhat.bag - boston.test)^2)\n#> [1] 25.75055\n###\nset.seed(1)\nrf.boston <- randomForest(medv ~ ., data = Boston,\n                          subset = train, mtry = 6, importance = TRUE)\nyhat.rf <- predict(rf.boston, newdata = Boston[-train, ])\nmean((yhat.rf - boston.test)^2)\n#> [1] 20.06644\n###\nimportance(rf.boston)\n#>           %IncMSE IncNodePurity\n#> crim    19.435587    1070.42307\n#> zn       3.091630      82.19257\n#> indus    6.140529     590.09536\n#> chas     1.370310      36.70356\n#> nox     13.263466     859.97091\n#> rm      35.094741    8270.33906\n#> age     15.144821     634.31220\n#> dis      9.163776     684.87953\n#> rad      4.793720      83.18719\n#> tax      4.410714     292.20949\n#> ptratio  8.612780     902.20190\n#> lstat   28.725343    5813.04833\n###\nvarImpPlot(rf.boston)\n\n## Boosting\n\n###\nlibrary(gbm)\n#> Loaded gbm 2.1.8\nset.seed(1)\nboost.boston <- gbm(medv ~ ., data = Boston[train, ],\n                    distribution = \"gaussian\", n.trees = 5000,\n                    interaction.depth = 4)\n###\nsummary(boost.boston)\n#>             var     rel.inf\n#> rm           rm 44.48249588\n#> lstat     lstat 32.70281223\n#> crim       crim  4.85109954\n#> dis         dis  4.48693083\n#> nox         nox  3.75222394\n#> age         age  3.19769210\n#> ptratio ptratio  2.81354826\n#> tax         tax  1.54417603\n#> indus     indus  1.03384666\n#> rad         rad  0.87625748\n#> zn           zn  0.16220479\n#> chas       chas  0.09671228\n###\nplot(boost.boston, i = \"rm\")\nplot(boost.boston, i = \"lstat\")\n###\nyhat.boost <- predict(boost.boston,\n                      newdata = Boston[-train, ], n.trees = 5000)\nmean((yhat.boost - boston.test)^2)\n#> [1] 18.39057\n###\nboost.boston <- gbm(medv ~ ., data = Boston[train, ],\n                    distribution = \"gaussian\", n.trees = 5000,\n                    interaction.depth = 4, shrinkage = 0.2, verbose = F)\nyhat.boost <- predict(boost.boston,\n                      newdata = Boston[-train, ], n.trees = 5000)\nmean((yhat.boost - boston.test)^2)\n#> [1] 16.54778\n\n## Bayesian Additive Regression Trees\n\n###\nlibrary(BART)\n#> Loading required package: nlme\n#> Loading required package: nnet\n#> Loading required package: survival\nx <- Boston[, 1:12]\ny <- Boston[, \"medv\"]\nxtrain <- x[train, ]\nytrain <- y[train]\nxtest <- x[-train, ]\nytest <- y[-train]\nset.seed(1)\nbartfit <- gbart(xtrain, ytrain, x.test = xtest)\n#> *****Calling gbart: type=1\n#> *****Data:\n#> data:n,p,np: 253, 12, 253\n#> y1,yn: 0.213439, -5.486561\n#> x1,x[n*p]: 0.109590, 20.080000\n#> xp1,xp[np*p]: 0.027310, 7.880000\n#> *****Number of Trees: 200\n#> *****Number of Cut Points: 100 ... 100\n#> *****burn,nd,thin: 100,1000,1\n#> *****Prior:beta,alpha,tau,nu,lambda,offset: 2,0.95,0.795495,3,3.71636,21.7866\n#> *****sigma: 4.367914\n#> *****w (weights): 1.000000 ... 1.000000\n#> *****Dirichlet:sparse,theta,omega,a,b,rho,augment: 0,0,1,0.5,1,12,0\n#> *****printevery: 100\n#> \n#> MCMC\n#> done 0 (out of 1100)\n#> done 100 (out of 1100)\n#> done 200 (out of 1100)\n#> done 300 (out of 1100)\n#> done 400 (out of 1100)\n#> done 500 (out of 1100)\n#> done 600 (out of 1100)\n#> done 700 (out of 1100)\n#> done 800 (out of 1100)\n#> done 900 (out of 1100)\n#> done 1000 (out of 1100)\n#> time: 4s\n#> trcnt,tecnt: 1000,1000\n###\nyhat.bart <- bartfit$yhat.test.mean\nmean((ytest - yhat.bart)^2)\n#> [1] 15.94718\n###\nord <- order(bartfit$varcount.mean, decreasing = T)\nbartfit$varcount.mean[ord]\n#>     nox   lstat     tax     rad      rm   indus    chas \n#>  22.952  21.329  21.250  20.781  19.890  19.825  19.051 \n#> ptratio     age      zn     dis    crim \n#>  18.976  18.274  15.952  14.457  11.007\n###"},{"path":"trees-9-the-oj-dataset.html","id":"trees-9-the-oj-dataset","chapter":"13 Trees #9 The OJ Dataset","heading":"13 Trees #9 The OJ Dataset","text":"problem involves OJ data set part ISLR package.train/test SplitQ: Create training set containing random sample 800 observations, test set containing remaining observations.: Since first time seeing dataset, quick overview: OJ dataset contains 1,070 purchases two brands orange juice (‘Citrus Hill’ ‘Minute Maid’), captured values Purchase variable (CH MM). remaining 17 predictors characteristics customer, product, store, etc. Throughout question basically tasked predicting orange juice customer purchased based statistics.create train test .Classification TreeQ: Fit tree training data, Purchase response variables predictors. Use summary() function produce summary statistics tree, describe results obtained. training error rate? many terminal nodes tree ?: classification tree 7 terminal nodes training error rate 18.38%.Despite 17 predictors dataset, three used splits. :tree() - Text InterpretationQ: Type name tree object order get detailed text output. Pick one terminal nodes, interpret information displayed.: print text output .Choosing node 11), terminal node marked *:First root node: 1) root 800 1064.00 CH ( 0.61750 0.38250 )means , root node, 800 observations, deviance 1064.00, overall prediction CH split 61.75% CH vs 38.25% MM.can see , root node, three splits take place produce terminal node labelled 11):Node 11) therefore subset purchases 0.142213 < LoyalCH < 0.5036 PriceDiff > 0.235. overall prediction CH, node seems quite impure 57.627% CH vs 42.373% MM.118 observations node, percentages know 68/118 CH 50/118 MM (demonstrated ).Based formula page 325 overall deviance classification tree \\[ (−2∑m∑knmklog(p^mk))\\] overall deviance sum m regions (terminal nodes). calculate can deviance node 11) using code :tree() reports number 160.80, testing nodes revealed ’s rounding result 4 significant figures.PlottingQ: Create plot tree, interpret results.:\nLoyalCH certainly important variable (top 3 nodes split variable), followed PriceDiff DiscCH. can see node 11) third terminal node (left → right).LoyalCH ranges 0 1, first split sends less loyal Citrus Hill (CH) orange juice left loyal right:plot(tree_model)\ntext(tree_model, pretty = 0, cex = 0.7)scored lowest Citrus Hill loyalty (LoyalCH < 0.142213) predicted buy Minute Maid (MM), isn’t surprising. slightly loyal CH (0.142213 < LoyalCH < 0.5036) still buy MM wasn’t much expensive (PriceDiff < 0.235), price difference large enough (CH much cheaper) end purchasing CH.far-right terminal node loyal CH (LoyalCH > 0.705699), unsurprising predicted purchase. slightly lower brand loyalty (0.5036 < LoyalCH < 0.705699) still purchase CH much cheaper (PriceDiff > 0.25), wasn’t sufficiently discounted (PriceDiff < 0.25 & DiscCH > 0.15). cases CH wasn’t much cheaper (PriceDiff < 0.25) wasn’t sufficiently discounted (DiscCH < 0.15), predicted purchase actually ended MM.much detailed explanation, summarized much higher level following way: people go brand loyal towards, edge cases (based discounts prices relative one another) can sway people usual brand loyalties.Test ErrorQ: Predict response test data, produce confusion matrix comparing test labels predicted test labels. test error rate?:confusion matrix unpruned regression tree:","code":"#> \n#> Attaching package: 'dplyr'\n#> The following objects are masked from 'package:stats':\n#> \n#>     filter, lag\n#> The following objects are masked from 'package:base':\n#> \n#>     intersect, setdiff, setequal, union\ndplyr::glimpse(OJ)\n#> Rows: 1,070\n#> Columns: 18\n#> $ Purchase       <fct> CH, CH, CH, MM, CH, CH, CH, CH, CH,…\n#> $ WeekofPurchase <dbl> 237, 239, 245, 227, 228, 230, 232, …\n#> $ StoreID        <dbl> 1, 1, 1, 1, 7, 7, 7, 7, 7, 7, 7, 7,…\n#> $ PriceCH        <dbl> 1.75, 1.75, 1.86, 1.69, 1.69, 1.69,…\n#> $ PriceMM        <dbl> 1.99, 1.99, 2.09, 1.69, 1.69, 1.99,…\n#> $ DiscCH         <dbl> 0.00, 0.00, 0.17, 0.00, 0.00, 0.00,…\n#> $ DiscMM         <dbl> 0.00, 0.30, 0.00, 0.00, 0.00, 0.00,…\n#> $ SpecialCH      <dbl> 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0,…\n#> $ SpecialMM      <dbl> 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0,…\n#> $ LoyalCH        <dbl> 0.500000, 0.600000, 0.680000, 0.400…\n#> $ SalePriceMM    <dbl> 1.99, 1.69, 2.09, 1.69, 1.69, 1.99,…\n#> $ SalePriceCH    <dbl> 1.75, 1.75, 1.69, 1.69, 1.69, 1.69,…\n#> $ PriceDiff      <dbl> 0.24, -0.06, 0.40, 0.00, 0.00, 0.30…\n#> $ Store7         <fct> No, No, No, No, Yes, Yes, Yes, Yes,…\n#> $ PctDiscMM      <dbl> 0.000000, 0.150754, 0.000000, 0.000…\n#> $ PctDiscCH      <dbl> 0.000000, 0.000000, 0.091398, 0.000…\n#> $ ListPriceDiff  <dbl> 0.24, 0.24, 0.23, 0.00, 0.00, 0.30,…\n#> $ STORE          <dbl> 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0,…\nset.seed(5)\ntrain_index <- sample(1:nrow(OJ), 800)\ntrain <- OJ[train_index, ]\ntest <- OJ[-train_index, ]\ntree_model <- tree(Purchase ~ ., train)\nsummary(tree_model)\n#> \n#> Classification tree:\n#> tree(formula = Purchase ~ ., data = train)\n#> Variables actually used in tree construction:\n#> [1] \"LoyalCH\"       \"PriceDiff\"     \"ListPriceDiff\"\n#> Number of terminal nodes:  9 \n#> Residual mean deviance:  0.7347 = 581.1 / 791 \n#> Misclassification error rate: 0.1662 = 133 / 800LoyalCH - Customer brand loyalty for CH\nPriceDiff - Sale price of MM less sale price of CH\nDiscCH - Discount offered for CH\ntree_model\n#> node), split, n, deviance, yval, (yprob)\n#>       * denotes terminal node\n#> \n#>  1) root 800 1068.00 CH ( 0.61250 0.38750 )  \n#>    2) LoyalCH < 0.5036 346  412.40 MM ( 0.28324 0.71676 )  \n#>      4) LoyalCH < 0.280875 164  125.50 MM ( 0.12805 0.87195 )  \n#>        8) LoyalCH < 0.0356415 56   10.03 MM ( 0.01786 0.98214 ) *\n#>        9) LoyalCH > 0.0356415 108  103.50 MM ( 0.18519 0.81481 ) *\n#>      5) LoyalCH > 0.280875 182  248.00 MM ( 0.42308 0.57692 )  \n#>       10) PriceDiff < 0.05 71   67.60 MM ( 0.18310 0.81690 ) *\n#>       11) PriceDiff > 0.05 111  151.30 CH ( 0.57658 0.42342 ) *\n#>    3) LoyalCH > 0.5036 454  362.00 CH ( 0.86344 0.13656 )  \n#>      6) PriceDiff < -0.39 31   40.32 MM ( 0.35484 0.64516 )  \n#>       12) LoyalCH < 0.638841 10    0.00 MM ( 0.00000 1.00000 ) *\n#>       13) LoyalCH > 0.638841 21   29.06 CH ( 0.52381 0.47619 ) *\n#>      7) PriceDiff > -0.39 423  273.70 CH ( 0.90071 0.09929 )  \n#>       14) LoyalCH < 0.705326 135  143.00 CH ( 0.77778 0.22222 )  \n#>         28) ListPriceDiff < 0.255 67   89.49 CH ( 0.61194 0.38806 ) *\n#>         29) ListPriceDiff > 0.255 68   30.43 CH ( 0.94118 0.05882 ) *\n#>       15) LoyalCH > 0.705326 288   99.77 CH ( 0.95833 0.04167 ) *A split at LoyalCH = 0.5036\nA split at LoyalCH = 0.142213\nA split at PriceDiff = 0.235\n\n 1) root 800 1064.00 CH ( 0.61750 0.38250 )  \n   2) LoyalCH < 0.5036 354  435.50 MM ( 0.30508 0.69492 )  \n     4) LoyalCH < 0.142213 100   45.39 MM ( 0.06000 0.94000 ) *\n     5) LoyalCH > 0.142213 254  342.20 MM ( 0.40157 0.59843 )  \n      10) PriceDiff < 0.235 136  153.00 MM ( 0.25000 0.75000 ) *\n      11) PriceDiff > 0.235 118  160.80 CH ( 0.57627 0.42373 ) *\ntrain %>%\n  filter(LoyalCH < 0.5036, \n         LoyalCH > 0.142213, \n         PriceDiff > 0.235) %>%\n  select(Purchase) %>% \n  table()\n#> Purchase\n#> CH MM \n#> 57 54\n-2 * (68 * log(68/118) + 50 * log(50/118))\n#> [1] 160.8262\ntest_pred <- predict(tree_model, test, type = \"class\")\ntable(test_pred, test_actual = test$Purchase)\n#>          test_actual\n#> test_pred  CH  MM\n#>        CH 148  32\n#>        MM  15  75"},{"path":"trees-9-the-oj-dataset.html","id":"test_actual","chapter":"13 Trees #9 The OJ Dataset","heading":"13.1 test_actual","text":"","code":""},{"path":"trees-9-the-oj-dataset.html","id":"test_pred-ch-mm","chapter":"13 Trees #9 The OJ Dataset","heading":"13.2 test_pred CH MM","text":"","code":""},{"path":"trees-9-the-oj-dataset.html","id":"ch-125-32","chapter":"13 Trees #9 The OJ Dataset","heading":"13.3 CH 125 32","text":"","code":""},{"path":"trees-9-the-oj-dataset.html","id":"mm-34-79","chapter":"13 Trees #9 The OJ Dataset","heading":"13.4 MM 34 79","text":"test error rate corresponding :1 - mean(test_pred == test$Purchase)","code":""},{"path":"trees-9-the-oj-dataset.html","id":"section","chapter":"13 Trees #9 The OJ Dataset","heading":"13.5 [1] 0.2444444","text":"CH common orange juice train , comparison, baseline classifier (predicted CH observations test) following error rate:1 - mean(test$Purchase == “CH”)","code":""},{"path":"trees-9-the-oj-dataset.html","id":"section-1","chapter":"13 Trees #9 The OJ Dataset","heading":"13.6 [1] 0.4111111","text":"Cost-Complexity PruningQ: Apply cv.tree() function training set order determine optimal tree size.:Since goal appears low test error, specify FUN = prune.misclass. indicates want classification error rate guide cross-validation pruning process, rather default cv.tree() function, deviance.CV Error PlotQ: Produce plot tree size x-axis cross-validated classification error rate y-axis.:plot . Note cv_tree_model\\(size number terminal nodes (1 means just root node splits), cv_tree_model\\)dev gives total number errors made --fold predictions cross-validation (specified FUN = prune.misclass - omitting mean reports deviance). can obtain cross-validation error rate.Best Tree - CV ErrorQ: tree size corresponds lowest cross-validated classification error rate?:sequence trees generated, trees sizes 4 7 cross-validation error. makes sense select parsimonious model 4 terminal nodes.Best Tree - SelectingQ: Produce pruned tree corresponding optimal tree size obtained using cross-validation. cross-validation lead selection pruned tree, create pruned tree five terminal nodes.:produce tree 4 terminal nodes. Interestingly cross-validation error full tree 7 terminal nodes, split LoyalCH achieve . added benefit simplifying interpretation part (d).Training Error ComparisonQ: Compare training error rates pruned unpruned trees. higher?:\ntraining error unpruned tree (7 terminal nodes):pruned tree (4 terminal nodes):training error pruned tree higher. isn’t surprising - expect training error tree monotonically decrease flexibility (number splits) increases.Test Error ComparisonQ: Compare test error rates pruned unpruned trees. higher?:test error unpruned tree:pruned tree:Now order reversed error higher unpruned tree.interesting cross-validation errors fact equal test error noticeably lower simpler tree. lot probably comes random variability working small dataset; using different random state CV folds train/test split likely change results (particularly decision trees high-variance approaches).","code":"\nset.seed(2)\ncv_tree_model <- cv.tree(tree_model, K = 10, FUN = prune.misclass)\ncv_tree_model\n#> $size\n#> [1] 9 6 5 3 2 1\n#> \n#> $dev\n#> [1] 149 149 149 173 172 310\n#> \n#> $k\n#> [1]  -Inf   0.0   1.0   8.5   9.0 150.0\n#> \n#> $method\n#> [1] \"misclass\"\n#> \n#> attr(,\"class\")\n#> [1] \"prune\"         \"tree.sequence\"\ndata.frame(size = cv_tree_model$size, CV_Error = cv_tree_model$dev / nrow(train)) %>%\n  mutate(min_CV_Error = as.numeric(min(CV_Error) == CV_Error)) %>%\n  ggplot(aes(x = size, y = CV_Error)) +\n  geom_line(col = \"grey55\") +\n  geom_point(size = 2, aes(col = factor(min_CV_Error))) +\n  scale_x_continuous(breaks = seq(1, 7), minor_breaks = NULL) +\n  scale_y_continuous(labels = scales::percent_format()) +\n  scale_color_manual(values = c(\"deepskyblue3\", \"green\")) +\n  theme(legend.position = \"none\") +\n  labs(title = \"OJ Dataset - Classification Tree\",\n       subtitle = \"Selecting tree 'size' (# of terminal nodes) using cross-validation\",\n       x = \"Tree Size\",\n       y = \"CV Error\")\npruned_tree_model <- prune.tree(tree_model, best = 4)\npruned_tree_model\n#> node), split, n, deviance, yval, (yprob)\n#>       * denotes terminal node\n#> \n#> 1) root 800 1068.00 CH ( 0.61250 0.38750 )  \n#>   2) LoyalCH < 0.5036 346  412.40 MM ( 0.28324 0.71676 )  \n#>     4) LoyalCH < 0.280875 164  125.50 MM ( 0.12805 0.87195 ) *\n#>     5) LoyalCH > 0.280875 182  248.00 MM ( 0.42308 0.57692 ) *\n#>   3) LoyalCH > 0.5036 454  362.00 CH ( 0.86344 0.13656 )  \n#>     6) PriceDiff < -0.39 31   40.32 MM ( 0.35484 0.64516 ) *\n#>     7) PriceDiff > -0.39 423  273.70 CH ( 0.90071 0.09929 ) *\nmean(predict(tree_model, type = \"class\") != train$Purchase)\n#> [1] 0.16625\nmean(predict(pruned_tree_model, type = \"class\") != train$Purchase)\n#> [1] 0.18875\nmean(predict(tree_model, type = \"class\", newdata = test) != test$Purchase)\n#> [1] 0.1740741\nmean(predict(pruned_tree_model, type = \"class\", newdata = test) != test$Purchase)\n#> [1] 0.2"},{"path":"project-e1.html","id":"project-e1","chapter":"14 Project (E1)","heading":"14 Project (E1)","text":"","code":""},{"path":"project-e1.html","id":"a-project-to-call-your-own","chapter":"14 Project (E1)","heading":"14.1 A project to call your own","text":"Pick dataset, dataset……something . first Analytics 2 project. Make us proud, nutshell. details .","code":""},{"path":"project-e1.html","id":"may-be-too-long-but-please-do-read","chapter":"14 Project (E1)","heading":"14.2 May be too long, but please do read","text":"project class consist analysis dataset \nchoosing. Please make sure ok choice. dataset may already exist,\nmay collect data using \nsurvey conducting experiment. can choose data based interests\nbased work courses research projects. goal project \ndemonstrate proficiency techniques covered class (\nbeyond, like) apply novel dataset meaningful way.","code":""},{"path":"project-e1.html","id":"data","chapter":"14 Project (E1)","heading":"14.3 Data","text":"order greatest chance success project important \nchoose manageable dataset. means data readily accessible large\nenough multiple relationships can explored. , dataset must least 50\nobservations 10 20 variables (exceptions can made must speak \nfirst). dataset’s variables include categorical variables, discrete numerical\nvariables, continuous numerical variables.analyses must done RStudio. using dataset comes format \nhaven’t encountered class, make sure able load RStudio \ncan tricky depending source. trouble ask help late.Reusing datasets class: reuse datasets used examples / homework \nclass.","code":""},{"path":"project-e1.html","id":"components","chapter":"14 Project (E1)","heading":"14.4 Components","text":"","code":""},{"path":"project-e1.html","id":"project-proposal","chapter":"14 Project (E1)","heading":"14.4.1 Project proposal","text":"draft introduction section project well \ndata analysis plan dataset. section 1\npage (excluding figures). can check print preview confirm length.write analysis including visuals must done using R Markdown.","code":""},{"path":"project-e1.html","id":"section-1---introduction","chapter":"14 Project (E1)","heading":"14.4.1.1 Section 1 - Introduction:","text":"introduction introduce general research\nquestion data (came , collected, \ncases, variables, etc.).","code":""},{"path":"project-e1.html","id":"section-2---data-analysis-plan","chapter":"14 Project (E1)","heading":"14.4.1.2 Section 2 - Data analysis plan:","text":"data analysis plan include:outcome (dependent, response, Y) predictor (independent, explanatory, X)\nvariables use answer question.comparison groups use, applicable.preliminary exploratory data analysis, including summary statistics \nvisualizations, along explanation help learn data.\n(can add later work project..)statistical method(s) believe useful answering question(s).\n(can update later work project.)Ideally use least two options: tree methods, linear regression,\nclassification (like logistic regression).results specific statistical methods needed support \nhypothesized answer?","code":""},{"path":"project-e1.html","id":"section-3---data","chapter":"14 Project (E1)","heading":"14.4.1.3 Section 3 - Data:","text":"yuor write , include enough details understand raw data looked like\nincluded.","code":""},{"path":"project-e1.html","id":"project","chapter":"14 Project (E1)","heading":"14.4.2 Project","text":"","code":""},{"path":"project-e1.html","id":"write-up","chapter":"14 Project (E1)","heading":"14.4.2.1 Write up","text":"providing description dataset research question \nintroduction use remainder write showcase arrived \nanswer / answers question using techniques learned \nclass (beyond, ’re feeling adventurous). goal exhaustive\ndata analysis .e., calculate every statistic procedure \nlearned every variable, rather let know proficient \nasking meaningful questions answering results data analysis, \nproficient using R, proficient interpreting \npresenting results. Focus methods help begin answer research\nquestions. apply every statistical procedure learned.\nAlso pay attention presentation. Neatness, coherency, clarity count.write must also include one two page conclusion discussion.\nrequire summary learned research\nquestion along statistical arguments supporting conclusions. Also\ncritique methods provide suggestions improving analysis.\nIssues pertaining reliability validity data, \nappropriateness statistical analysis discussed . \nparagraph differently able start \nproject next going continue\nwork project also included.project open ended. create kind compelling\nvisualization(s) data R.limit tools packages may use, sticking packages learned class (ISLR R4DS)\nrequired. need visualize data . single high quality\nvisualization receive much higher grade large number poor quality\nvisualizations.finalize write , make sure chunks turned \necho = FALSE. Exception: want \nhighlight something specific piece code, ’re welcomed show\nportion. [See : also want copy raw .Rmd file just html output.]can add sections see fit project make sure\nsection called Introduction beginning section called\nConclusion end. rest !","code":""},{"path":"project-e1.html","id":"presentation","chapter":"14 Project (E1)","heading":"14.4.2.2 Presentation","text":"10 minutes maximum.can use software like final presentation, including R Markdown\ncreate slides. isn’t limit many slides can use, just \ntime limit (10 minutes total). Perhaps try ioslides beamer. presentation\njust account everything \ntried (“, , etc.”), instead convey \nchoices made, , found.","code":""},{"path":"project-e1.html","id":"delivarables","chapter":"14 Project (E1)","heading":"14.4.2.3 Delivarables","text":"submission includeRMarkdown file (formatted clearly present code results)HTML fileDataset(s) (csv RData format, /data folder)Presentation (using Keynote/PowerPoint/Google Slides, export PDF put repo, /presentation folder)Style format count assignment, please take time make\nsure everything looks good data code properly formated.","code":""},{"path":"project-e1.html","id":"grading","chapter":"14 Project (E1)","heading":"14.5 Grading","text":"Grading project take account following:Content - quality research /policy question relevancy\ndata questions?Correctness - statistical procedures carried explained correctly?Writing Presentation - quality statistical presentation,\nwriting, explanations?Creativity Critical Thought - project carefully thought ? \nlimitations carefully considered? appear time effort went \nplanning implementation project?general breakdown scoring follows:90%-100% - Outstanding effort. Student understands apply statistical\nconcepts, can put results cogent argument, can identify weaknesses \nargument, can clearly communicate results others.80%-89% - Good effort. Student understands concepts, puts together\nadequate argument, identifies weaknesses argument, communicates\nresults clearly others.70%-79% - Passing effort. Student misunderstanding concepts several\nareas, trouble putting results together cogent argument, communication\nresults sometimes unclear.60%-69% - Struggling effort. Student making effort, misunderstanding\nmany concepts unable put together cogent argument. Communication\nresults unclear.60% - Student making sufficient effort.Late penalty:Late, within 24 hours due date/time: -20% (applies written portion, option presentation later)later: credit","code":""}]
